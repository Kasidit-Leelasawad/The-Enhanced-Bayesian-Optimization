# -*- coding: utf-8 -*-
"""BO3+NSGA2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d3lsoPgm3y2rTsd1XD5oL1nAufkxr-aS
"""

# Installing necessary packages and algorithms

!pip install scikit-quant
!pip install -U pymoo
!pip install gpytorch
!pip install disjoint-set
!pip install HEBO

from skquant.opt import minimize as sk_minimize

# other python packages
import copy
import numpy as np
import scipy
from numpy import mean
from scipy.optimize import (
    minimize,
    differential_evolution,
    basinhopping,
    direct,
    Bounds
)
from tqdm import tqdm
import matplotlib.pyplot as plt

"""# Algorithms

The following are some of the algorithms to be benchmarked. While the majority are optimization packages, we have also included a few examples coded from scratch.

## Random Search
"""

###############################################################################
#                      Random Search (Legacy)                                 #
###############################################################################

def random_search_legacy(f, n_params, bounds_rs, n_iters):
    """
    A naive optimization routine that randomly samples the allowed space
    and returns the best value.

    Args:
        f (callable): Objective function to minimize.
        n_params (int): Number of parameters.
        bounds_rs (np.ndarray): (n_params, 2) array with [lower, upper] bounds.
        n_iters (int): Number of random samples/iterations.

    Returns:
        tuple: (best_function_value, best_parameter_array)
    """
    local_x   = np.zeros((n_params, n_iters))  # Points sampled
    local_val = np.zeros(n_iters)            # Function values sampled

    bounds_range = bounds_rs[:, 1] - bounds_rs[:, 0]
    bounds_bias  = bounds_rs[:, 0]

    for sample_i in range(n_iters):
        x_trial = np.random.uniform(0, 1, n_params) * bounds_range + bounds_bias
        local_x[:, sample_i] = x_trial
        local_val[sample_i] = f(x_trial)

    # Choose the best
    min_index = np.argmin(local_val)
    f_best = local_val[min_index]
    x_best = local_x[:, min_index]

    return f_best, x_best

"""## SnobFit"""

####################
# SnobFit algorithm #
####################

def opt_SnobFit(f, x_dim, bounds, iter_tot):
    '''
    params: parameters that define the rbf model
    X:      matrix of previous datapoints
    '''

    n_rs = int(min(100,max(iter_tot*0.20,5)))       # iterations to find good starting point

    # evaluate first point
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)
    iter_          = iter_tot - n_rs

    # Convert bounds to list of tuples for sk_minimize
    bounds_list = [(bounds[i, 0], bounds[i, 1]) for i in range(x_dim)]

    result, history = \
    sk_minimize(f, x_best, bounds_list, iter_, method='SnobFit')

    return result.optpar, result.optval

"""## Bobyqa"""

####################
# Bobyqa  algorithm #
####################

def opt_Bobyqa(f, x_dim, bounds, iter_tot):
    '''
    params: parameters that define the rbf model
    X:      matrix of previous datapoints
    '''

    n_rs = int(min(100, max(iter_tot*.05,5)))       # iterations to find good starting point

    # evaluate first point
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)
    iter_          = iter_tot - n_rs

    result, history = \
    sk_minimize(f, x_best, bounds, iter_, method='Bobyqa')

    return result.optpar, result.optval

"""## Powell"""

###############################################################################
#                                Powell                                       #
###############################################################################

def opt_powell(f, x_dim, bounds, iter_tot):
    """
    Minimization using the Powell method from SciPy.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    # Attempt to find a good starting point via random search
    n_rs = int(min(100, max(iter_tot * 0.2, 5)))
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)

    # Remaining evaluations after random search
    remaining_evals = iter_tot - n_rs

    # Powell minimization
    opt_res = scipy_minimize(
        f, x_best,
        bounds=bounds,
        method="Powell",
        options={"maxfev": remaining_evals}
    )

    return opt_res.x, opt_res.fun

"""## COBYLA"""

###############################################################################
#                                COBYLA                                      #
###############################################################################

def opt_cobyla(f, x_dim, bounds, iter_tot):
    """
    Minimization using the COBYLA method from SciPy.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    # Attempt to find a good starting point via random search
    n_rs = int(min(100, max(iter_tot * 0.05, 5)))
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)

    # Remaining evaluations after random search
    remaining_evals = iter_tot - n_rs

    # COBYLA minimization
    opt_res = scipy_minimize(
        f, x_best,
        method="COBYLA",
        options={"maxfev": remaining_evals}
    )

    return opt_res.x, opt_res.fun

"""## Bayesian optimization"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.gaussian_process.kernels import Kernel, Hyperparameter, Matern
from sklearn.gaussian_process import GaussianProcessRegressor
import warnings
from torch.quasirandom import SobolEngine
warnings.filterwarnings('ignore')

!pip install deap

# Import NSGA-II libraries
try:
    from pymoo.algorithms.moo.nsga2 import NSGA2
    from pymoo.core.problem import Problem
    from pymoo.optimize import minimize
    from pymoo.operators.crossover.sbx import SBX
    from pymoo.operators.mutation.pm import PM
    from pymoo.operators.sampling.rnd import FloatRandomSampling
    PYMOO_AVAILABLE = True
    print("‚úÖ pymoo NSGA-II successfully imported")
except ImportError:
    PYMOO_AVAILABLE = False
    print("‚ùå pymoo not available. Install with: pip install pymoo")

try:
    from deap import base, creator, tools, algorithms
    import random
    DEAP_AVAILABLE = True
    print("‚úÖ DEAP successfully imported")
except ImportError:
    DEAP_AVAILABLE = False
    print("‚ùå DEAP not available. Install with: pip install deap")


class TrustRegionManager:
    """Enhanced Trust Region Manager with TuRBO-M style multi-region management."""

    def __init__(self, dim, n_trust_regions=3, length_init=1, length_min=0.25, length_max=1.6):
        self.dim = dim
        self.n_trust_regions = n_trust_regions
        self.length_init = length_init
        self.length_min = length_min
        self.length_max = length_max

        # Success and failure tolerances (More responsive)
        self.succtol = 3
        self.failtol = 8

        # Initialize trust region parameters
        self.length = np.ones(n_trust_regions) * length_init
        self.succcount = np.zeros(n_trust_regions, dtype=int)
        self.failcount = np.zeros(n_trust_regions, dtype=int)
        self.centers = None
        self.is_initialized = False

        # Track which trust region was used for each evaluation
        self._idx = np.zeros((0, 1), dtype=int)

    def initialize_centers(self, X, fX):
        """Initialize trust region centers using best points."""
        n_points = len(X)
        if n_points < self.n_trust_regions:
            best_idx = np.argmin(fX)
            centers_idx = [best_idx] * self.n_trust_regions
        else:
            # K-means++ style initialization
            centers_idx = [np.argmin(fX)]  # Start with best point

            for _ in range(1, self.n_trust_regions):
                distances = np.inf * np.ones(n_points)
                for idx in centers_idx:
                    dist = np.linalg.norm(X - X[idx], axis=1)
                    distances = np.minimum(distances, dist)

                probabilities = distances**2
                if probabilities.sum() > 0:
                    probabilities /= probabilities.sum()
                    next_center = np.random.choice(n_points, p=probabilities)
                else:
                    available_indices = [i for i in range(n_points) if i not in centers_idx]
                    next_center = np.random.choice(available_indices) if available_indices else centers_idx[0]
                centers_idx.append(next_center)

        self.centers = X[centers_idx].copy()
        self.is_initialized = True
        print(f"üéØ Trust regions initialized with {self.n_trust_regions} centers")
        return self.centers

    def update_trust_region(self, tr_idx, fX_next, fX_history, global_best=None):
        """Update trust region based on improvement."""
        if tr_idx >= self.n_trust_regions:
            print(f"   ‚ö†Ô∏è TR-{tr_idx} index out of bounds")
            return False, False, False, None

        # Get minimum value in this trust region's history
        tr_history_idx = np.where(self._idx[:, 0] == tr_idx)[0]
        fX_min = fX_history[tr_history_idx].min() if len(tr_history_idx) > 0 else np.inf

        new_min = fX_next.min()

        # Enhanced success criteria
        improvement_threshold = 1e-3 * max(1.0, abs(fX_min))
        tr_improvement = new_min < fX_min - improvement_threshold
        global_improvement = global_best is None or new_min < global_best
        significant_tr_improvement = new_min < fX_min * 0.95
        success = significant_tr_improvement or global_improvement

        if success:
            self.succcount[tr_idx] += 1
            self.failcount[tr_idx] = 0
            print(f"   ‚úì TR-{tr_idx} success (count={self.succcount[tr_idx]})")
        else:
            self.succcount[tr_idx] = 0
            self.failcount[tr_idx] += len(fX_next)
            print(f"   ‚úó TR-{tr_idx} failure (count={self.failcount[tr_idx]})")

        # Check for adaptive adjustments
        adaptive_adjustments_made = False
        adjustment_type = None

        if self.succcount[tr_idx] == self.succtol:
            old_length = self.length[tr_idx]
            self.length[tr_idx] = min(2.0 * self.length[tr_idx], self.length_max)
            print(f"üìà TR-{tr_idx} EXPANDED: {old_length:.4f} ‚Üí {self.length[tr_idx]:.4f}")
            adaptive_adjustments_made = True
            adjustment_type = "success"
            self.succcount[tr_idx] = 0

        elif self.failcount[tr_idx] >= self.failtol:
            old_length = self.length[tr_idx]
            self.length[tr_idx] /= 2.0
            print(f"üìâ TR-{tr_idx} SHRUNK: {old_length:.4f} ‚Üí {self.length[tr_idx]:.4f}")
            adaptive_adjustments_made = True
            adjustment_type = "failure"
            self.failcount[tr_idx] = 0

        restart_needed = self.length[tr_idx] < self.length_min
        return success, restart_needed, adaptive_adjustments_made, adjustment_type

    def restart_trust_region(self, tr_idx, X_history, y_history):
        """Restart a specific trust region."""
        print(f"üîÑ Restarting TR-{tr_idx}")
        self.length[tr_idx] = self.length_init
        self.succcount[tr_idx] = 0
        self.failcount[tr_idx] = 0

        # Mark old points as inactive
        tr_idx_mask = self._idx[:, 0] == tr_idx
        self._idx[tr_idx_mask, 0] = -1

        # Find new center from global best
        best_idx = np.argmin(y_history)
        self.centers[tr_idx] = X_history[best_idx].copy()

    def add_evaluation_idx(self, idx_batch):
        """Add trust region indices for new evaluations."""
        self._idx = np.vstack((self._idx, idx_batch))

    def get_trust_region_bounds(self, tr_idx, lb, ub):
        """Get bounds for a specific trust region."""
        if not self.is_initialized:
            return lb, ub

        center = self.centers[tr_idx]
        length = self.length[tr_idx]

        tr_lb = np.clip(center - length * (ub - lb) / 2, lb, ub)
        tr_ub = np.clip(center + length * (ub - lb) / 2, lb, ub)

        return tr_lb, tr_ub

    def get_status_summary(self):
        """Get a summary of trust region status."""
        if not self.is_initialized:
            return "Trust regions not initialized"

        summary = f"\nüìä Trust Region Status:\n"
        for i in range(self.n_trust_regions):
            active_points = np.sum(self._idx[:, 0] == i)
            summary += f"  TR-{i}: length={self.length[i]:.4f}, "
            summary += f"points={active_points}, "
            summary += f"succ={self.succcount[i]}, fail={self.failcount[i]}\n"
        return summary


class GPWrapper:
    """Wrapper for GP to match interface."""
    def __init__(self, gp):
        self.gp = gp

    def predict(self, X, return_std=False, return_cov=False):
        return self.gp.predict(X, return_std=return_std, return_cov=return_cov)

    def fit(self, X, y):
        self.gp.fit(X, y)
        return self

    def __getattr__(self, name):
        return getattr(self.gp, name)


class NSGA2EvolutionarySearch:
    """NSGA-II based optimizer for acquisition function optimization in BO."""

    def __init__(self, dim, bounds, int_var, cont_var, value_maps, backend='auto'):
        self.dim = dim
        self.bounds = bounds
        self.int_var = int_var
        self.cont_var = cont_var
        self.value_maps = value_maps

        # Auto-select backend
        if backend == 'auto':
            if PYMOO_AVAILABLE:
                self.backend = 'pymoo'
            elif DEAP_AVAILABLE:
                self.backend = 'deap'
            else:
                raise ImportError("Neither pymoo nor DEAP is available")
        else:
            self.backend = backend

        if self.backend == 'pymoo' and not PYMOO_AVAILABLE:
            raise ImportError("pymoo not available")
        if self.backend == 'deap' and not DEAP_AVAILABLE:
            raise ImportError("DEAP not available")

        print(f"‚úÖ Using {self.backend} for NSGA-II optimization")

    def optimize_acquisition(self, acq_func, n_suggestions=1, n_candidates=100, n_generations=20):
        """Use NSGA-II to optimize acquisition function."""
        if self.backend == 'pymoo':
            return self._optimize_pymoo(acq_func, n_suggestions, n_candidates, n_generations)
        else:
            return self._optimize_deap(acq_func, n_suggestions, n_candidates, n_generations)

    def _optimize_pymoo(self, acq_func, n_suggestions, n_candidates, n_generations):
        """Optimize using pymoo NSGA-II."""

        class AcquisitionProblem(Problem):
            def __init__(problem_self, outer_self, acq_func):
                problem_self.outer_self = outer_self
                problem_self.acq_func = acq_func

                # Set bounds
                xl, xu = [], []
                for i in range(problem_self.outer_self.dim):
                    if i in problem_self.outer_self.int_var:
                        xl.append(0)
                        xu.append(len(problem_self.outer_self.value_maps[i]) - 1)
                    else:
                        xl.append(problem_self.outer_self.bounds[i][0])
                        xu.append(problem_self.outer_self.bounds[i][1])

                # Test to determine number of objectives
                test_x = problem_self._create_test_array()
                test_result = problem_self.acq_func(test_x)
                n_obj = test_result.shape[1] if len(test_result.shape) > 1 else 1

                super().__init__(
                    n_var=problem_self.outer_self.dim,
                    n_obj=n_obj,
                    xl=np.array(xl),
                    xu=np.array(xu)
                )

            def _create_test_array(problem_self):
                """Create test array to determine n_objectives."""
                x = np.zeros((1, problem_self.outer_self.dim))
                for i in range(problem_self.outer_self.dim):
                    if i in problem_self.outer_self.int_var:
                        x[0, i] = len(problem_self.outer_self.value_maps[i]) // 2
                    else:
                        x[0, i] = (problem_self.outer_self.bounds[i][0] + problem_self.outer_self.bounds[i][1]) / 2
                return x

            def _evaluate(problem_self, X, out, *args, **kwargs):
                """Evaluate acquisition function for pymoo."""
                X_real = X.copy()
                for i in problem_self.outer_self.int_var:
                    X_real[:, i] = np.clip(np.round(X[:, i]), 0, len(problem_self.outer_self.value_maps[i]) - 1)

                acq_values = problem_self.acq_func(X_real)
                out["F"] = -acq_values

        problem = AcquisitionProblem(self, acq_func)

        algorithm = NSGA2(
            pop_size=n_candidates,
            sampling=FloatRandomSampling(),
            crossover=SBX(prob=0.9, eta=15),
            mutation=PM(prob=1.0/self.dim, eta=20)
        )

        res = minimize(
            problem,
            algorithm,
            ('n_gen', n_generations),
            verbose=False
        )

        if res.X is None:
            print("‚ö†Ô∏è NSGA-II optimization failed, using random candidates")
            return self._generate_random_candidates(n_suggestions)

        # Get best solutions
        if len(res.X.shape) == 1:
            best_solutions = res.X.reshape(1, -1)
        else:
            best_solutions = res.X[:min(n_suggestions, len(res.X))]

        # Convert to our parameter format
        candidates = []
        for x in best_solutions:
            row = []
            for i in range(self.dim):
                if i in self.int_var:
                    idx = int(np.clip(np.round(x[i]), 0, len(self.value_maps[i]) - 1))
                    row.append(idx)
                else:
                    row.append(x[i])
            candidates.append(row)

        # Fill remaining slots if needed
        while len(candidates) < n_suggestions:
            candidates.extend(self._generate_random_candidates(1))

        return candidates[:n_suggestions]

    def _optimize_deap(self, acq_func, n_suggestions, n_candidates, n_generations):
        """Optimize using DEAP NSGA-II."""

        # Clean up any existing creator classes
        if hasattr(creator, "FitnessMulti"):
            del creator.FitnessMulti
        if hasattr(creator, "Individual"):
            del creator.Individual

        # Setup DEAP
        creator.create("FitnessMulti", base.Fitness, weights=(1.0,) * 10)
        creator.create("Individual", list, fitness=creator.FitnessMulti)

        toolbox = base.Toolbox()

        # Register attribute generators
        for i in range(self.dim):
            if i in self.int_var:
                toolbox.register(f"attr_{i}", random.randint, 0, len(self.value_maps[i]) - 1)
            else:
                toolbox.register(f"attr_{i}", random.uniform, self.bounds[i][0], self.bounds[i][1])

        # Individual generator
        def create_individual():
            ind = creator.Individual()
            for i in range(self.dim):
                ind.append(getattr(toolbox, f"attr_{i}")())
            return ind

        toolbox.register("individual", create_individual)
        toolbox.register("population", tools.initRepeat, list, toolbox.individual)

        # Evaluation function
        def evaluate(individual):
            x = np.array(individual).reshape(1, -1)
            acq_values = acq_func(x)
            if hasattr(acq_values, 'shape') and len(acq_values.shape) > 1:
                return tuple(acq_values[0])
            else:
                return (float(acq_values),)

        toolbox.register("evaluate", evaluate)

        # Genetic operators with bounds
        low_bounds, up_bounds = [], []
        for i in range(self.dim):
            if i in self.int_var:
                low_bounds.append(0)
                up_bounds.append(len(self.value_maps[i]) - 1)
            else:
                low_bounds.append(self.bounds[i][0])
                up_bounds.append(self.bounds[i][1])

        toolbox.register("mate", tools.cxSimulatedBinaryBounded,
                        low=low_bounds, up=up_bounds, eta=20.0)
        toolbox.register("mutate", tools.mutPolynomialBounded,
                        low=low_bounds, up=up_bounds, eta=20.0, indpb=1.0/self.dim)
        toolbox.register("select", tools.selNSGA2)

        # Run evolution
        population = toolbox.population(n=n_candidates)

        # Evaluate initial population
        fitnesses = list(map(toolbox.evaluate, population))
        for ind, fit in zip(population, fitnesses):
            ind.fitness.values = fit

        # Evolution loop
        for gen in range(n_generations):
            offspring = algorithms.varAnd(population, toolbox, cxpb=0.9, mutpb=1.0)
            fitnesses = list(map(toolbox.evaluate, offspring))
            for ind, fit in zip(offspring, fitnesses):
                ind.fitness.values = fit
            population = toolbox.select(population + offspring, n_candidates)

        # Get best individuals
        best_individuals = tools.selBest(population, n_suggestions)

        # Convert to our format
        candidates = []
        for ind in best_individuals:
            row = []
            for i in range(self.dim):
                if i in self.int_var:
                    row.append(int(np.clip(ind[i], 0, len(self.value_maps[i]) - 1)))
                else:
                    row.append(ind[i])
            candidates.append(row)

        # Clean up
        del creator.FitnessMulti
        del creator.Individual

        return candidates

    def _generate_random_candidates(self, n_candidates):
        """Generate random candidates as fallback."""
        candidates = []
        for _ in range(n_candidates):
            x = []
            for i in range(self.dim):
                if i in self.int_var:
                    x.append(np.random.randint(0, len(self.value_maps[i])))
                else:
                    x.append(np.random.uniform(self.bounds[i][0], self.bounds[i][1]))
            candidates.append(x)
        return candidates


class EnhancedBayesianOptimizer:
    """Enhanced Bayesian Optimizer with TuRBO-M style trust regions and NSGA-II."""

    def __init__(self, test_func, dim, int_var=None, bounds=None, step=0.2,
                 max_evals=20, scaler_type='minmax', n_trust_regions=3,
                 use_thompson_sampling=True, batch_size=4, verbose=1,
                 use_evolution=True, beta=2.0, evolution_backend='auto'):

        self.test_func = test_func
        self.dim = dim
        self.step = step
        self.max_evals = max_evals
        self.scaler_type = scaler_type
        self.batch_size = batch_size
        self.use_thompson_sampling = use_thompson_sampling
        self.verbose = verbose
        self.use_evolution = use_evolution
        self.beta = beta
        self.evolution_backend = evolution_backend

        # Initialize components
        self._init_variable_types(int_var)
        self._init_bounds_and_maps(bounds)
        self._init_scaler()
        self._init_trust_region_manager(n_trust_regions)
        self._init_surrogate()
        self._init_optimizer()

        # Initialize history
        self.history_x = []
        self.history_y = []
        self.history_x_normalized = []
        self.current_best_y = np.inf
        self.current_best_x = None

        # Enhanced batch size parameters
        self.batch_size_min = 2
        self.batch_size_max = 10
        self.batch_size_step = 1
        self.batch_size_history = [self.batch_size]

        # Trust region count parameters
        self.n_tr_min = 1
        self.n_tr_max = 4
        self.tr_adjust_step = 1

        # Weight ratio (used in acquisition function)
        self.weight_ucb = 0.5
        self.weight_ts = 0.5

        # Performance tracking
        self.no_improvement_count = 0
        self.improvement_threshold = 1e-6
        self.stagnation_limit = 15

        self._selected_tr_indices = None

    def _print_current_status(self, iteration):
        """Print current parameter values."""
        print(f"\nüîß ITERATION {iteration} - CURRENT PARAMETERS:")
        print(f"   üìä Batch Size: {self.batch_size}")
        print(f"   üéØ Number of TRs: {self.trust_region_mgr.n_trust_regions}")
        print(f"   üìà No improvement count: {self.no_improvement_count}/{self.stagnation_limit}")

    def _init_variable_types(self, int_var):
        """Initialize continuous and integer variable indices."""
        if int_var is None:
            int_amount = max(1, int(self.dim * 0.5))
            np.random.seed(42)
            self.int_var = sorted(np.random.choice(self.dim, int_amount, replace=False).tolist())
            np.random.seed()
        else:
            self.int_var = sorted(int_var)
        self.cont_var = [i for i in range(self.dim) if i not in self.int_var]

    def _init_bounds_and_maps(self, bounds):
        """Initialize bounds and value mappings."""
        if bounds is None:
            self.bounds = np.array([[0, 1]] * self.dim)
        else:
            self.bounds = bounds

        # Create value maps for integer variables
        self.value_maps = {}
        for i in self.int_var:
            lb, ub = self.bounds[i]
            values = np.round(np.arange(lb, ub + self.step, self.step), 10)
            self.value_maps[i] = values.tolist()

        # Create optimization bounds
        self.opt_bounds = []
        for i in range(self.dim):
            if i in self.int_var:
                ub_idx = len(self.value_maps[i]) - 1
                self.opt_bounds.append((0, ub_idx))
            else:
                self.opt_bounds.append(tuple(self.bounds[i]))

    def _init_scaler(self):
        """Initialize input scaler."""
        if self.scaler_type == 'minmax':
            self.scaler = MinMaxScaler(feature_range=(0, 1))
        elif self.scaler_type == 'standard':
            self.scaler = StandardScaler()
        else:
            raise ValueError(f"Unknown scaler type: {self.scaler_type}")

        # Generate representative samples for fitting
        n_fit_samples = min(1000, 10**min(self.dim, 4))
        fit_points = []
        for _ in range(n_fit_samples):
            point = []
            for i in range(self.dim):
                if i in self.int_var:
                    point.append(np.random.choice(self.value_maps[i]))
                else:
                    point.append(np.random.uniform(self.bounds[i][0], self.bounds[i][1]))
            fit_points.append(point)

        self.scaler.fit(np.array(fit_points))

    def _init_trust_region_manager(self, n_trust_regions):
        """Initialize trust region manager."""
        self.trust_region_mgr = TrustRegionManager(
            dim=self.dim,
            n_trust_regions=n_trust_regions
        )

    def _init_surrogate(self):
        """Initialize Gaussian Process surrogate model."""
        self.kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-5, 1e3), nu=2.5)
        gp = GaussianProcessRegressor(
            kernel=self.kernel,
            alpha=1e-6,
            normalize_y=True,
            n_restarts_optimizer=5,
            random_state=42
        )
        self.surrogate = GPWrapper(gp)

    def _init_optimizer(self):
        """Initialize NSGA-II evolution optimizer."""
        if self.use_evolution:
            try:
                self.evolution_search = NSGA2EvolutionarySearch(
                    dim=self.dim,
                    bounds=self.bounds,
                    int_var=self.int_var,
                    cont_var=self.cont_var,
                    value_maps=self.value_maps,
                    backend=self.evolution_backend
                )
                print("‚úÖ NSGA-II Evolution optimizer initialized successfully")
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: NSGA-II Evolution initialization failed: {e}")
                self.use_evolution = False
                self.evolution_search = None

    def normalize_inputs(self, X):
        """Normalize inputs using fitted scaler."""
        return self.scaler.transform(X)

    def denormalize_inputs(self, X_normalized):
        """Denormalize inputs using fitted scaler."""
        return self.scaler.inverse_transform(X_normalized)

    def map_params(self, params_df):
        """Map integer indices to actual values."""
        real_params = params_df.copy()
        for i in self.int_var:
            col = f'x{i}'
            real_params[col] = real_params[col].apply(
                lambda idx: self.value_maps[i][int(np.clip(round(idx), 0, len(self.value_maps[i])-1))]
            )
        return real_params

    def _generate_candidates_for_trust_region(self, n_candidates, tr_idx):
        """Generate candidates for a specific trust region."""
        if not self.trust_region_mgr.is_initialized:
            return self._generate_random_candidates(n_candidates)

        # Get trust region bounds
        lb = np.zeros(self.dim)
        ub = np.ones(self.dim)
        tr_lb_norm, tr_ub_norm = self.trust_region_mgr.get_trust_region_bounds(tr_idx, lb, ub)

        if self.use_evolution and self.evolution_search is not None:
            try:
                # Create acquisition function for this trust region
                def tr_acquisition(X_array):
                    X_real = []
                    for row in X_array:
                        x_real = []
                        for i in range(self.dim):
                            if i in self.int_var:
                                idx = int(np.clip(row[i], 0, len(self.value_maps[i]) - 1))
                                x_real.append(self.value_maps[i][idx])
                            else:
                                x_real.append(row[i])
                        X_real.append(x_real)
                    X_real = np.array(X_real)
                    X_norm = self.normalize_inputs(X_real)
                    X_norm = np.clip(X_norm, tr_lb_norm, tr_ub_norm)
                    return self._multi_objective_acquisition(X_norm)

                candidates = self.evolution_search.optimize_acquisition(
                    acq_func=tr_acquisition,
                    n_suggestions=n_candidates,
                    n_candidates=min(100, n_candidates * 10),
                    n_generations=20
                )
                return candidates

            except Exception as e:
                if self.verbose >= 1:
                    print(f"   ‚ö†Ô∏è NSGA-II failed for TR-{tr_idx}: {e}")

        # Fallback: Sobol-based generation
        try:
            sobol = SobolEngine(dimension=self.dim, scramble=True)
            pert = sobol.draw(n_candidates).numpy()
        except:
            pert = np.random.rand(n_candidates, self.dim)

        # Scale to trust region
        pert = tr_lb_norm + (tr_ub_norm - tr_lb_norm) * pert

        # Create perturbation mask
        prob_perturb = min(20.0 / self.dim, 1.0)
        mask = np.random.rand(n_candidates, self.dim) <= prob_perturb
        ind = np.where(np.sum(mask, axis=1) == 0)[0]
        mask[ind, np.random.randint(0, self.dim, size=len(ind))] = 1

        # Create candidates
        center = self.trust_region_mgr.centers[tr_idx]
        X_cand = np.tile(center, (n_candidates, 1))
        X_cand[mask] = pert[mask]

        # Convert to parameter space
        X_cand_real = self.denormalize_inputs(X_cand)
        candidates = []
        for row in X_cand_real:
            x = []
            for i in range(self.dim):
                if i in self.int_var:
                    values = self.value_maps[i]
                    val = min(values, key=lambda v: abs(v - row[i]))
                    idx = values.index(val)
                    x.append(idx)
                else:
                    x.append(row[i])
            candidates.append(x)

        return candidates

    def _generate_random_candidates(self, n_candidates):
        """Generate random candidates within bounds."""
        candidates = []
        for _ in range(n_candidates):
            x = []
            for i in range(self.dim):
                lb, ub = self.opt_bounds[i]
                if i in self.int_var:
                    val = np.random.randint(lb, ub + 1)
                else:
                    val = np.random.uniform(lb, ub)
                x.append(val)
            candidates.append(x)
        return candidates

    def _multi_objective_acquisition(self, candidates_norm):
        """Compute multi-objective acquisition values."""
        iteration = len(self.history_y)

        # Dynamic acquisition weighting based on progress (3 phases)
        progress = iteration / self.max_evals
        if progress < 0.3:  # First 30% - More exploration
            self.weight_ucb = 0.3
            self.weight_ts = 0.7
        elif progress < 0.7:  # Middle 40% - Balanced
            self.weight_ucb = 0.5
            self.weight_ts = 0.5
        else:  # Final 30% - More exploitation
            self.weight_ucb = 0.95
            self.weight_ts = 0.05

        candidates_norm = np.atleast_2d(candidates_norm)

        # GP-UCB with fixed beta
        mean, std = self.surrogate.predict(candidates_norm, return_std=True)
        ucb_values = -(mean + self.beta * std)

        # Thompson Sampling
        if self.use_thompson_sampling:
            ts_values = self._thompson_acquisition(candidates_norm)
            return np.column_stack([ucb_values, ts_values])
        else:
            return ucb_values.reshape(-1, 1)

    def _thompson_acquisition(self, X_candidates, n_samples=5):
        """Thompson Sampling acquisition function."""
        gp = self.surrogate.gp
        try:
            mean, cov = gp.predict(X_candidates, return_cov=True)
            cov += 1e-8 * np.eye(len(X_candidates))

            samples = []
            rng = np.random.RandomState(None)
            for _ in range(n_samples):
                try:
                    sample = rng.multivariate_normal(mean, cov)
                except np.linalg.LinAlgError:
                    std = np.sqrt(np.diag(cov))
                    sample = rng.normal(mean, std)
                samples.append(sample)

            samples = np.array(samples)
            return -np.min(samples, axis=0)

        except Exception:
            mean, std = gp.predict(X_candidates, return_std=True)
            return -mean.ravel()

    def _select_candidates_multi_tr(self, all_candidates, all_acq_values, all_tr_indices, n_suggestions):
        """Select best candidates from multiple trust regions."""
        if all_acq_values.shape[1] > 1:
            combined_acq = (
                self.weight_ucb * all_acq_values[:, 0] +
                self.weight_ts * all_acq_values[:, 1]
            )
        else:
            combined_acq = all_acq_values.ravel()

        # Select top candidates
        X_next = []
        idx_next = []
        selected_indices = set()

        for _ in range(n_suggestions):
            best_idx = None
            best_val = -np.inf

            for i in range(len(combined_acq)):
                if i not in selected_indices and combined_acq[i] > best_val:
                    best_val = combined_acq[i]
                    best_idx = i

            if best_idx is not None:
                X_next.append(all_candidates[best_idx])
                idx_next.append(all_tr_indices[best_idx])
                selected_indices.add(best_idx)

        return np.array(X_next), np.array(idx_next).reshape(-1, 1)

    def suggest(self, n_suggestions=1):
        """Suggest next points using multi-trust-region approach."""
        # Initial random sampling
        if len(self.history_x_normalized) < 2:
            candidates = self._generate_space_filling_candidates(n_suggestions)
            self._selected_tr_indices = np.zeros(n_suggestions, dtype=int)
            return pd.DataFrame(candidates, columns=[f'x{i}' for i in range(self.dim)])

        # Initialize trust regions if needed
        if not self.trust_region_mgr.is_initialized and len(self.history_x_normalized) >= 3:
            X_hist = np.vstack(self.history_x_normalized)
            y_hist = np.vstack(self.history_y).ravel()
            self.trust_region_mgr.initialize_centers(X_hist, y_hist)

        # Generate candidates from multiple trust regions
        all_candidates = []
        all_acq_values = []
        all_tr_indices = []

        base_candidates = max(30, 150 // self.trust_region_mgr.n_trust_regions)

        # Only sample from active trust regions
        active_trs = []
        for tr_idx in range(self.trust_region_mgr.n_trust_regions):
            if self.trust_region_mgr.length[tr_idx] >= self.trust_region_mgr.length_min:
                active_trs.append(tr_idx)

        if not active_trs:
            active_trs = list(range(self.trust_region_mgr.n_trust_regions))

        for tr_idx in active_trs:
            n_cand_per_tr = base_candidates
            if tr_idx < len(self.trust_region_mgr.succcount):
                if self.trust_region_mgr.succcount[tr_idx] > 0:
                    n_cand_per_tr = int(base_candidates * 1.5)

            candidates = self._generate_candidates_for_trust_region(n_cand_per_tr, tr_idx)

            if candidates:
                candidates_df = pd.DataFrame(candidates, columns=[f'x{i}' for i in range(self.dim)])
                real_candidates = self.map_params(candidates_df).values
                candidates_norm = self.normalize_inputs(real_candidates)
                acq_values = self._multi_objective_acquisition(candidates_norm)

                all_candidates.extend(candidates)
                all_acq_values.extend(acq_values)
                all_tr_indices.extend([tr_idx] * len(candidates))

        if len(all_candidates) == 0:
            candidates = self._generate_random_candidates(n_suggestions)
            self._selected_tr_indices = np.zeros(n_suggestions, dtype=int)
            return pd.DataFrame(candidates, columns=[f'x{i}' for i in range(self.dim)])

        all_candidates = np.array(all_candidates)
        all_acq_values = np.array(all_acq_values)
        all_tr_indices = np.array(all_tr_indices)

        X_next, idx_next = self._select_candidates_multi_tr(
            all_candidates, all_acq_values, all_tr_indices, n_suggestions
        )

        self._selected_tr_indices = idx_next.ravel()

        return pd.DataFrame(X_next, columns=[f'x{i}' for i in range(self.dim)])

    def _generate_space_filling_candidates(self, n_candidates):
        """Generate space-filling candidates for initial exploration."""
        try:
            sobol = SobolEngine(dimension=self.dim, scramble=True)
            sobol_points = sobol.draw(n_candidates).numpy()

            candidates = []
            for point in sobol_points:
                x = []
                for i in range(self.dim):
                    if i in self.int_var:
                        n_values = len(self.value_maps[i])
                        idx = int(point[i] * n_values)
                        idx = min(idx, n_values - 1)
                        x.append(idx)
                    else:
                        lb, ub = self.opt_bounds[i]
                        val = lb + point[i] * (ub - lb)
                        x.append(val)
                candidates.append(x)
            return candidates
        except:
            return self._generate_random_candidates(n_candidates)

    def evaluate(self, params_df):
        """Evaluate the test function at given parameters."""
        real_params = self.map_params(params_df)
        print(f"\nüìã Evaluating {len(real_params)} points:")
        print(real_params.to_string(index=False, float_format="%.5f"))

        results = []
        for _, row in real_params.iterrows():
            result = self.test_func.eval(row.values.astype(np.float64))
            results.append(result)

        return np.array(results).reshape(-1, 1)

    def observe(self, x_df, y):
        """Observe evaluation results and update model."""
        # Update history
        self.history_x.append(x_df)
        self.history_y.append(y)

        # Track improvements
        current_min = np.min(y)
        old_best = self.current_best_y

        if current_min < self.current_best_y:
            improvement = self.current_best_y - current_min
            self.current_best_y = current_min
            best_idx = np.argmin(y.ravel())
            self.current_best_x = self.map_params(x_df.iloc[[best_idx]])

            if improvement > self.improvement_threshold:
                self.no_improvement_count = 0
                print(f"   üéâ New best found! Improvement: {improvement:.6f}")
            else:
                self.no_improvement_count += 1
        else:
            self.no_improvement_count += 1

        # Normalize and store
        real_params = self.map_params(x_df)
        normalized_params = self.normalize_inputs(real_params.values)
        self.history_x_normalized.append(normalized_params)

        # Retrain GP
        X_train = np.vstack(self.history_x_normalized)
        y_train = np.vstack(self.history_y).ravel()
        self.surrogate.fit(X_train, y_train)

        # Update trust regions
        if self.trust_region_mgr.is_initialized and self._selected_tr_indices is not None:
            idx_batch = self._selected_tr_indices.reshape(-1, 1)
            self.trust_region_mgr.add_evaluation_idx(idx_batch)

            unique_tr = np.unique(self._selected_tr_indices)
            for tr_idx in unique_tr:
                if tr_idx >= self.trust_region_mgr.n_trust_regions:
                    continue

                tr_mask = self._selected_tr_indices == tr_idx
                y_tr = y[tr_mask]

                # Update center if improvement found
                if len(y_tr) > 0 and y_tr.min() < self.current_best_y:
                    best_local_idx = np.argmin(y_tr)
                    global_idx = np.where(tr_mask)[0][best_local_idx]
                    new_center = normalized_params[global_idx]
                    self.trust_region_mgr.centers[tr_idx] = new_center
                    print(f"   ‚≠ê TR-{tr_idx} center updated to new best")

                result = self.trust_region_mgr.update_trust_region(
                    tr_idx, y_tr, y_train, global_best=self.current_best_y
                )

                if len(result) == 4:
                    success, restart_needed, adaptive_adjustments_made, adjustment_type = result
                else:
                    success, restart_needed = result
                    adaptive_adjustments_made = False
                    adjustment_type = None

                if adaptive_adjustments_made:
                    self._apply_enhanced_adaptive_adjustments(tr_idx, adjustment_type)

                if restart_needed:
                    self.trust_region_mgr.restart_trust_region(tr_idx, X_train, y_train)

    def _apply_enhanced_adaptive_adjustments(self, tr_idx, adjustment_type):
        """Apply adaptive adjustments when TR length scaling occurs."""
        print(f"\nüîÑ TR-{tr_idx} triggered adaptive adjustments ({adjustment_type})")

        if adjustment_type == "success":
            # Increase batch_size
            old_batch = self.batch_size
            self.batch_size = min(self.batch_size_max, self.batch_size + self.batch_size_step)
            if self.batch_size != old_batch:
                print(f"   üìà Increased batch_size: {old_batch} ‚Üí {self.batch_size}")

            # Reduce number of TRs
            old_n_tr = self.trust_region_mgr.n_trust_regions
            new_n_tr = max(self.n_tr_min, old_n_tr - self.tr_adjust_step)
            if new_n_tr != old_n_tr:
                self._adjust_trust_regions(new_n_tr)
                print(f"   üîª Decreased number of TRs: {old_n_tr} ‚Üí {new_n_tr}")

        elif adjustment_type == "failure":
            # Reduce batch_size
            old_batch = self.batch_size
            self.batch_size = max(self.batch_size_min, self.batch_size - self.batch_size_step)
            if self.batch_size != old_batch:
                print(f"   üìâ Reduced batch_size: {old_batch} ‚Üí {self.batch_size}")

            # Increase number of TRs
            old_n_tr = self.trust_region_mgr.n_trust_regions
            new_n_tr = min(self.n_tr_max, old_n_tr + self.tr_adjust_step)
            if new_n_tr != old_n_tr:
                self._adjust_trust_regions(new_n_tr)
                print(f"   üî∫ Increased number of TRs: {old_n_tr} ‚Üí {new_n_tr}")

    def _adjust_trust_regions(self, new_n_tr):
        """Adjust number of trust regions."""
        self.trust_region_mgr.n_trust_regions = new_n_tr
        self.trust_region_mgr.length = np.ones(new_n_tr) * self.trust_region_mgr.length_init
        self.trust_region_mgr.succcount = np.zeros(new_n_tr, dtype=int)
        self.trust_region_mgr.failcount = np.zeros(new_n_tr, dtype=int)
        self.trust_region_mgr.centers = None
        self.trust_region_mgr.is_initialized = False
        self.trust_region_mgr._idx = np.zeros((0, 1), dtype=int)

    def _restart_all_trust_regions(self):
        """Restart all trust regions when optimization stagnates."""
        print("üîÑ Restarting all trust regions due to stagnation")

        for tr_idx in range(self.trust_region_mgr.n_trust_regions):
            self.trust_region_mgr.length[tr_idx] = self.trust_region_mgr.length_init
            self.trust_region_mgr.succcount[tr_idx] = 0
            self.trust_region_mgr.failcount[tr_idx] = 0

        if len(self.history_x_normalized) > 0:
            X_hist = np.vstack(self.history_x_normalized)
            y_hist = np.vstack(self.history_y).ravel()
            self.trust_region_mgr.initialize_centers(X_hist, y_hist)

        self.batch_size = min(self.batch_size_max, self.batch_size + 2)
        print(f"   üìà Increased batch_size for exploration: {self.batch_size}")

    def optimize(self):
        """Run the optimization loop."""
        print(f"üöÄ Starting Enhanced Bayesian Optimization with NSGA-II Evolution")
        print(f"   - Dimensions: {self.dim} ({len(self.cont_var)} continuous, {len(self.int_var)} integer)")
        print(f"   - Max evaluations: {self.max_evals}")
        print(f"   - Initial batch size: {self.batch_size}")
        print(f"   - Initial trust regions: {self.trust_region_mgr.n_trust_regions}")
        print(f"   - NSGA-II Evolution: {'Enabled' if self.use_evolution else 'Disabled'}")

        n_eval = 0
        iteration = 0

        # Main optimization loop
        while n_eval < self.max_evals:
            iteration += 1
            self._print_current_status(iteration)

            # Check for early stopping
            if self.no_improvement_count >= self.stagnation_limit:
                self._restart_all_trust_regions()
                self.no_improvement_count = 0

            n_suggest = min(self.batch_size, self.max_evals - n_eval)

            suggestions = self.suggest(n_suggestions=n_suggest)
            y_values = self.evaluate(suggestions)
            self.observe(suggestions, y_values)

            n_eval += n_suggest

            print(f"\nüìä Evaluation {n_eval}/{self.max_evals}")
            print(f"   Best value: {self.current_best_y:.6f}")
            print(f"   Current batch: {[f'{val:.6f}' for val in y_values.ravel()]}")

        # Final summary
        print(f"\n‚úÖ Optimization completed!")
        print(f"   - Total evaluations: {n_eval}")
        print(f"   - Best value found: {self.current_best_y:.6f}")
        print(f"   - Final batch size: {self.batch_size}")
        print(f"   - Final TR count: {self.trust_region_mgr.n_trust_regions}")
        print(self.trust_region_mgr.get_status_summary())

        if self.current_best_x is not None:
            print(f"\nüèÜ Best parameters:")
            print(self.current_best_x.to_string(index=False, float_format="%.6f"))

        return self.current_best_x, self.current_best_y

def opt_BO(
    test_func_eval,
    dim,
    bounds,
    max_evals,
    int_var=None,
    int_ratio=0.2,
    step=1,
    batch_size=3,
    return_optimizer=False,
    random_seed=None,
    scaler_type='minmax',
    n_trust_regions=3,
    use_thompson_sampling=True,
    verbose=1,
    batch_size_min=2,
    batch_size_max=10,
    n_tr_min=1,
    n_tr_max=4,
    weight_ucb_init=0.5,
    weight_ts_init=0.5,
    stagnation_limit=10,
    adjustment_cooldown=5,
    use_evolution=True,
    beta=2.0,
    evolution_backend='auto',
):
    """
    Enhanced Bayesian Optimization with TuRBO-M style trust regions and NSGA-II Evolution.

    Parameters:
    -----------
    test_func_eval : callable
        Objective function to minimize
    dim : int
        Number of dimensions
    bounds : np.ndarray
        Bounds for each dimension (shape: dim x 2)
    max_evals : int
        Maximum number of function evaluations
    int_var : list, optional
        List of indices for integer variables
    int_ratio : float
        Ratio of integer variables if int_var not specified
    step : float
        Step size for integer variables
    batch_size : int
        Initial batch size for parallel evaluations
    return_optimizer : bool
        Whether to return the optimizer object
    random_seed : int, optional
        Random seed for reproducibility
    scaler_type : str
        Type of scaler ('minmax' or 'standard')
    n_trust_regions : int
        Initial number of trust regions
    use_thompson_sampling : bool
        Whether to use Thompson Sampling
    verbose : int
        Verbosity level (0=quiet, 1=normal, 2=detailed)
    batch_size_min : int
        Minimum batch size
    batch_size_max : int
        Maximum batch size
    n_tr_min : int
        Minimum number of trust regions
    n_tr_max : int
        Maximum number of trust regions
    weight_ucb_init : float
        Initial weight for UCB acquisition
    weight_ts_init : float
        Initial weight for Thompson Sampling
    stagnation_limit : int
        Number of iterations without improvement before restart
    adjustment_cooldown : int
        Cooldown period between adjustments (not used in current implementation)
    use_evolution : bool
        Whether to use NSGA-II Evolution optimizer
    beta : float
        Fixed beta value for GP-UCB
    evolution_backend : str
        Backend for NSGA-II ('auto', 'pymoo', or 'deap')

    Returns:
    --------
    best_x : pd.DataFrame
        Best parameters found
    best_y : float
        Best objective value found
    optimizer : EnhancedBayesianOptimizer (optional)
        The optimizer object if return_optimizer=True
    """
    import numpy as np

    class WrappedTestFunc:
        def __init__(self, evaluator):
            self.eval = evaluator.eval if hasattr(evaluator, 'eval') else evaluator

    wrapped_func = WrappedTestFunc(test_func_eval)

    if random_seed is not None:
        np.random.seed(random_seed)

    if int_var is None:
        int_amount = max(1, int(dim * int_ratio))
        int_var = sorted(np.random.choice(dim, int_amount, replace=False).tolist())

    if bounds.shape != (dim, 2):
        raise ValueError(f"[opt_BO] Bounds shape {bounds.shape} does not match dim={dim}")
    if not all(0 <= idx < dim for idx in int_var):
        raise ValueError(f"[opt_BO] Integer variable indices {int_var} must be in range [0, {dim-1}]")

    print(f"üîß Initializing Enhanced Bayesian Optimizer with NSGA-II Evolution")
    print(f"   - Dimensions: {dim}")
    print(f"   - Integer vars: {int_var}")
    print(f"   - Continuous vars: {[i for i in range(dim) if i not in int_var]}")
    print(f"   - Max evaluations: {max_evals}")
    print(f"   - Initial batch size: {batch_size} (range: {batch_size_min}-{batch_size_max})")
    print(f"   - Initial trust regions: {n_trust_regions} (range: {n_tr_min}-{n_tr_max})")
    print(f"   - Initial weight ratio: UCB={weight_ucb_init:.2f}, TS={weight_ts_init:.2f}")
    print(f"   - Early stopping: {stagnation_limit} iterations without improvement")
    print(f"   - Thompson Sampling: {use_thompson_sampling}")
    print(f"   - NSGA-II Evolution: {use_evolution} (backend: {evolution_backend})")
    print(f"   - Beta (GP-UCB): {beta:.3f}")

    # Create optimizer
    optimizer = EnhancedBayesianOptimizer(
        test_func=wrapped_func,
        dim=dim,
        int_var=int_var,
        bounds=bounds,
        step=step,
        max_evals=max_evals,
        batch_size=batch_size,
        scaler_type=scaler_type,
        n_trust_regions=n_trust_regions,
        use_thompson_sampling=use_thompson_sampling,
        verbose=verbose,
        use_evolution=use_evolution,
        beta=beta,
        evolution_backend=evolution_backend,
    )

    # Set adaptive parameters
    optimizer.batch_size_min = batch_size_min
    optimizer.batch_size_max = batch_size_max
    optimizer.n_tr_min = n_tr_min
    optimizer.n_tr_max = n_tr_max
    optimizer.weight_ucb = weight_ucb_init
    optimizer.weight_ts = weight_ts_init
    optimizer.stagnation_limit = stagnation_limit

    # Note: adjustment_cooldown is not used in the current implementation
    if hasattr(optimizer, 'adjustment_cooldown'):
        optimizer.adjustment_cooldown = adjustment_cooldown

    print(f"‚úÖ Enhanced adaptive parameters configured:")
    print(f"   - Batch size range: {batch_size_min}-{batch_size_max}")
    print(f"   - TR count range: {n_tr_min}-{n_tr_max}")
    print(f"   - Adaptive TR thresholds: Success={optimizer.trust_region_mgr.succtol}, Failure={optimizer.trust_region_mgr.failtol}")
    print(f"   - Stagnation limit: {stagnation_limit}")
    print(f"   - Beta value: {beta:.3f}")
    print(f"   - Evolution backend: {evolution_backend}")

    # Run optimization
    best_x, best_y = optimizer.optimize()

    print(f"\nüèÅ Enhanced optimization with NSGA-II Evolution complete!")
    print(f"   - Total evaluations: {sum(len(y) for y in optimizer.history_y)}")
    print(f"   - Best value: {best_y:.6f}")
    print(f"   - Final batch size: {optimizer.batch_size}")
    print(f"   - Final TR count: {optimizer.trust_region_mgr.n_trust_regions}")
    print(f"   - Final weight ratio: UCB={optimizer.weight_ucb:.2f}, TS={optimizer.weight_ts:.2f}")
    print(f"   - Beta value used: {beta:.3f}")
    print(f"   - Stagnation periods: {optimizer.no_improvement_count}")

    if return_optimizer:
        return best_x, best_y, optimizer
    else:
        return best_x, best_y

"""## Nelder-Mead"""

###############################################################################
#                             Nelder-Mead                                    #
###############################################################################
from scipy.optimize import minimize as scipy_minimize
def opt_nelder_mead(f, x_dim, bounds, iter_tot):
    """
    Minimization using the Nelder-Mead method from SciPy.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    # Attempt to find a good starting point via random search
    n_rs = int(min(100, max(iter_tot * 0.2, 5)))
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)

    # Remaining evaluations after random search
    remaining_evals = iter_tot - n_rs

    # Nelder-Mead minimization
    opt_res = scipy_minimize(
        f, x_best,
        method="Nelder-Mead",
        options={"maxfev": remaining_evals}
    )

    return opt_res.x, opt_res.fun

"""## Differential Evolution (DE)"""

###############################################################################
#                        Differential Evolution (DE)                          #
###############################################################################

def opt_de(f, x_dim, bounds, iter_tot):
    """
    Minimization using SciPy's Differential Evolution.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    # Convert (x_dim, 2) array to list of tuples for DE
    bounds_list = [(float(b[0]), float(b[1])) for b in bounds]

    # Estimate popsize and maxiter
    popsize_ = int(min(100, max(iter_tot * 0.05, 5)))
    maxiter_ = int(iter_tot / popsize_) + 3

    # Differential Evolution
    opt_res = differential_evolution(
        f,
        bounds_list,
        maxiter=maxiter_,
        popsize=popsize_,
    )

    return opt_res.x, opt_res.fun

"""## Basinhopping"""

###############################################################################
#                               Basinhopping                                  #
###############################################################################

def opt_basinhopping(f, x_dim, bounds, iter_tot):
    """
    Minimization using SciPy's Basinhopping.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    # Attempt to find a good starting point via random search
    n_rs = int(min(100, max(iter_tot * 0.05, 5)))
    f_best, x_best = random_search_legacy(f, x_dim, bounds, n_rs)

    # Remaining evaluations after random search
    remaining_evals = iter_tot - n_rs
    minimizer_kwargs = {"method": "BFGS"}

    # Approx. the number of iterations we can do
    niter_ = int(iter_tot / 3)

    opt_res = basinhopping(
        f,
        x_best,
        minimizer_kwargs=minimizer_kwargs,
        niter=niter_
    )

    return opt_res.x, opt_res.fun

"""## DIRECT"""

###############################################################################
#                                DIRECT                                       #
###############################################################################

def opt_direct(f, x_dim, bounds, iter_tot):
    """
    Minimization using the 'direct' algorithm (if available via SciPy).

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with lower/upper bounds.
        iter_tot (int): Total budget of function evaluations.

    Returns:
        tuple: (best_parameters, best_function_value)
    """
    bounds_list = [(float(b[0]), float(b[1])) for b in bounds]
    opt_res = direct(f, bounds_list, maxfun=iter_tot)
    return opt_res.x, opt_res.fun

"""## Particle Swarm Optimisation with Domain reduction"""

###############################################################################
#                     Particle Swarm Optimization (PSO)                       #
###############################################################################

def domain_reduction_v2(x_dic, n_particles):
    """
    Reduces search space by taking the min/max among all best positions.
    """
    new_domain = x_dic['0']['best_position']
    for i_x in range(1, n_particles):
        new_domain = np.vstack((new_domain, x_dic[str(i_x)]['best_position']))

    max_bound = np.max(new_domain, axis=0)
    min_bound = np.min(new_domain, axis=0)

    return np.vstack((min_bound, max_bound)).T

def sample_uniform_params_v2(n_particles_, bounds_range, bounds_bias):
    """
    Uniform sampling of parameters in a given bounds_range and bounds_bias.
    """
    return (
        np.random.uniform(0, 1, (n_particles_, bounds_range.shape[0]))
        * bounds_range + bounds_bias
    )

def sample_uniform_params_log_v2(n_particles_, bounds_range, bounds_bias):
    """
    Uniform sampling in logarithmic space.
    """
    log_bounds_range = np.log(bounds_range)
    rand_particles_log = np.random.uniform(
        0, 1, (n_particles_, bounds_range.shape[0])
    ) * log_bounds_range

    return np.exp(rand_particles_log) + bounds_bias

def calculate_v_log_v2(w_k, c1, c2, v_p, x_p, x_local_best, x_global_best):
    """
    Calculates particle velocity in log-space for exploration-exploitation.
    """
    diff_local = x_local_best - x_p
    diff_global = x_global_best - x_p

    abs_diff_local = np.abs(diff_local)
    abs_diff_global = np.abs(diff_global)

    log_abs_diff_local = np.log(abs_diff_local + 1e-10)
    log_abs_diff_global = np.log(abs_diff_global + 1e-10)

    rand_local = np.random.uniform(0, 1, x_p.shape)
    rand_global = np.random.uniform(0, 1, x_p.shape)

    scaled_log_local = rand_local * log_abs_diff_local
    scaled_log_global = rand_global * log_abs_diff_global

    scaled_local = np.sign(diff_local) * np.exp(scaled_log_local)
    scaled_global = np.sign(diff_global) * np.exp(scaled_log_global)

    v_new = w_k * v_p + c1 * scaled_local + c2 * scaled_global
    return v_new

def calculate_v_v2(w_k, c1, c2, v_p, x_p, x_local_best, x_global_best):
    """
    Calculates particle velocity using the standard PSO formula.
    """
    inertia = w_k * v_p
    local_comp = c1 * np.random.uniform(0, 1, x_p.shape) * (x_local_best - x_p)
    global_comp = c2 * np.random.uniform(0, 1, x_p.shape) * (x_global_best - x_p)

    return inertia + local_comp + global_comp

def pso_v2(n_particles, evals, bounds, func, x_good=None, f_good=None):
    """
    Basic PSO algorithm to find the minimum of a given function.
    """
    if x_good is None:
        x_good = []
    if f_good is None:
        f_good = np.inf

    eval_count = 0
    best_reward = np.inf

    c1, c2 = 2.8, 1.3
    c3 = c1 + c2
    # Velocity weighting factor (inertia)
    w_k = 2.0 / abs(2.0 - c3 - np.sqrt(c3**2 - 4 * c3))

    bounds_range = bounds[:, 1] - bounds[:, 0]
    bounds_bias = bounds[:, 0]
    lower_bounds = bounds[:, 0]
    upper_bounds = bounds[:, 1]

    v_lb = -abs(bounds_range) * 0.75
    v_ub = abs(bounds_range) * 0.75

    data_pso = {'R_list': np.zeros(evals)}
    particle_dic = {}

    # If we have a good starting point
    if len(x_good) > 0:
        n_p = n_particles - 1
        particle_dic[str(n_p)] = {
            'particle_x': x_good.copy(),
            'best_position': x_good.copy(),
            'best_obj': f_good,
            'vel': sample_uniform_params_v2(1, v_ub - v_lb, v_lb)[0]
        }
        best_reward = f_good
        best_particle = x_good.copy()
        eval_count += 1
        nn = 1
    else:
        nn = 0
        best_particle = None

    # Initialize other particles
    for particle_i in range(n_particles - nn):
        if np.random.uniform(0, 1) > 0.5:
            sample_uniform_ = sample_uniform_params_v2
            sample_velocity_ = calculate_v_v2
        else:
            sample_uniform_ = sample_uniform_params_log_v2
            sample_velocity_ = calculate_v_log_v2

        part_x = sample_uniform_(1, bounds_range, bounds_bias)[0]
        particle_dic[str(particle_i)] = {
            'particle_x': part_x.copy(),
            'best_position': part_x.copy(),
            'best_obj': np.inf,
            'vel': sample_uniform_(1, v_ub - v_lb, v_lb)[0]
        }

        data_pso['R_list'][eval_count] = func(particle_dic[str(particle_i)]['particle_x'])
        particle_dic[str(particle_i)]['best_obj'] = data_pso['R_list'][eval_count]

        if data_pso['R_list'][eval_count] < best_reward:
            best_reward = data_pso['R_list'][eval_count]
            best_particle = particle_dic[str(particle_i)]['particle_x'].copy()

        eval_count += 1

    # Main PSO loop
    while eval_count < evals:
        for p_i in range(n_particles):
            if np.random.uniform(0, 1) > 0.5:
                sample_velocity_ = calculate_v_v2
            else:
                sample_velocity_ = calculate_v_log_v2

            p_dict = particle_dic[str(p_i)]
            p_dict['vel'] = sample_velocity_(
                w_k, c1, c2,
                p_dict['vel'],
                p_dict['particle_x'],
                p_dict['best_position'],
                best_particle
            )

            # Update position and clip to bounds
            p_dict['particle_x'] = p_dict['particle_x'] + p_dict['vel']
            p_dict['particle_x'] = np.clip(
                p_dict['particle_x'],
                lower_bounds,
                upper_bounds
            )

            # Evaluate
            data_pso['R_list'][eval_count] = func(p_dict['particle_x'])

            if data_pso['R_list'][eval_count] < p_dict['best_obj']:
                p_dict['best_obj'] = data_pso['R_list'][eval_count]
                p_dict['best_position'] = p_dict['particle_x'].copy()

                if p_dict['best_obj'] < best_reward:
                    best_reward = p_dict['best_obj']
                    best_particle = p_dict['particle_x'].copy()

            eval_count += 1
            if eval_count >= evals:
                break

    return best_particle, best_reward, data_pso, particle_dic

###############################################################################
#      PSO with Domain Reduction + Local Search (PSO_red_v2)                  #
###############################################################################

def pso_red_v2(n_particles, evals, bounds, func):
    """
    A repeated PSO approach where after each cycle the domain is reduced.
    Finally, a local search is performed for further refinement.
    """
    iter_per_cycle = int(evals / 4)

    # First cycle
    best_x, best_f, data_pso, x_dic = pso_v2(n_particles, iter_per_cycle, bounds, func)

    # Two additional cycles of domain reduction + PSO
    for _ in range(2):
        new_b = domain_reduction_v2(x_dic, n_particles)
        best_x, best_f, data_pso, x_dic = pso_v2(
            n_particles, iter_per_cycle, new_b, func, x_good=best_x, f_good=best_f
        )

    # Local search
    n_x    = best_x.shape[0]
    new_b  = domain_reduction_v2(x_dic, n_particles)
    iter_t = int(iter_per_cycle * (n_particles / n_x + 1))

    best_x, best_f, _ = random_local_search(func, best_f, best_x, iter_t, n_x, new_b)

    return best_x, best_f, data_pso, x_dic

###############################################################################
#        Wrapper: Repeated PSO with Local Search (pso_red_f_v2)              #
###############################################################################

def pso_red_f_v2(f, x_dim, bounds, iter_tot):
    """
    High-level function that runs repeated PSO with local search.

    Args:
        f (callable): Objective function.
        x_dim (int): Dimensionality of the search space.
        bounds (np.ndarray): (x_dim, 2) array with [lower, upper] bounds.
        iter_tot (int): Total number of function evaluations.

    Returns:
        tuple: (best_x, best_f) where best_x are the best parameters found,
               and best_f is the best objective value.
    """
    min_gens = 10
    cycles_number = 4
    iter_for_particles = int(iter_tot / cycles_number / min_gens)

    n_particles = max(5, min(iter_for_particles, x_dim * 10))

    best_x, best_f, _, _ = pso_red_v2(n_particles, iter_tot, bounds, f)
    return best_x, best_f

"""## Stochastic local search"""

###############################################################################
#                    Random Search (Stochastic Search)                        #
###############################################################################

def random_search(f, n_params, bounds, n_iters):
    """
    Perform a naive random search over the given parameter space.

    Args:
        f (callable): Objective function to minimize.
        n_params (int): Number of parameters.
        bounds (np.ndarray): (n_params, 2) array with [lower, upper] bounds.
        n_iters (int): Number of samples to evaluate.

    Returns:
        tuple: (best_value, best_params)
    """
    bounds_range = bounds[:, 1] - bounds[:, 0]
    bounds_bias = bounds[:, 0]

    sampled_points = (
        np.random.uniform(0, 1, (n_iters, n_params)) * bounds_range + bounds_bias
    )
    sampled_values = np.array([f(point) for point in sampled_points])

    min_index = np.argmin(sampled_values)
    best_value = sampled_values[min_index]
    best_params = sampled_points[min_index]

    return best_value, best_params

###############################################################################
#                              Ball Sampling                                  #
###############################################################################

def ball_sampling(n_dims, radius):
    """
    Sample randomly within a ball of a given radius in n_dims space.
    """
    u = np.random.normal(0, 1, n_dims)
    norm_u = np.linalg.norm(u)
    r = np.random.uniform() ** (1.0 / n_dims)

    return r * u / norm_u * radius * 2.0

###############################################################################
#                           Log-Uniform Sampling                              #
###############################################################################

def log_uniform_sample(size, min_val=1e-8, max_val=1.0):
    """
    Generate samples uniformly distributed in log-space
    between min_val and max_val.
    """
    log_min = np.log(min_val)
    log_max = np.log(max_val)
    log_samples = np.random.uniform(log_min, log_max, size)

    samples = np.exp(log_samples)
    signs = np.random.choice([-1, 1], size=size)
    samples *= signs

    return samples

###############################################################################
#                        Random Local Search (RLS)                            #
###############################################################################

def random_local_search(f, best_value, best_params, n_samples, n_params, bounds):
    """
    Perform a local random search to refine the solution.

    Args:
        f (callable): Objective function.
        best_value (float): Current best function value.
        best_params (np.ndarray): Current best parameters.
        n_samples (int): Number of local samples to evaluate.
        n_params (int): Dimensionality.
        bounds (np.ndarray): (n_params, 2) array with [lower, upper] bounds.

    Returns:
        tuple: (best_value, best_params, radius_list)
    """
    radius = (bounds[:, 1] - bounds[:, 0]) * 0.5
    samples_per_iteration = n_params*2
    n_iterations = n_samples // samples_per_iteration
    gamma = 0.95
    radius_list = []

    for i_iter in range(n_iterations+1):
        radius_list.append(np.mean(radius))

        if i_iter % 2 == 0:
            # Sample in log space
            sampled_points = np.array([
                best_params + log_uniform_sample(n_params) * radius
                for _ in range(samples_per_iteration)
            ])
        else:
            # Sample in a ball
            sampled_points = np.array([
                best_params + ball_sampling(n_params, radius)
                for _ in range(samples_per_iteration)
            ])

        sampled_points = np.clip(sampled_points, bounds[:, 0], bounds[:, 1])
        sampled_values = np.array([f(point) for point in sampled_points])

        min_index = np.argmin(sampled_values)
        local_best_value = sampled_values[min_index]
        local_best_params = sampled_points[min_index]

        if local_best_value < best_value:
            best_value = local_best_value
            best_params = local_best_params
        else:
            radius *= gamma

    return best_value, best_params, radius_list

###############################################################################
#                  Stochastic Search (Global + Local)                         #
###############################################################################

def SS_alg(f, n_params, bounds, n_iters):
    """
    Perform a stochastic search combining global random search and local random search.

    Args:
        f (callable): Objective function to minimize.
        n_params (int): Number of parameters.
        bounds (np.ndarray): (n_params, 2) array with [lower, upper] bounds.
        n_iters (int): Total number of function evaluations.

    Returns:
        tuple: (best_params, best_value, radius_list)
    """
    n_random_search_iters = int(n_iters * 0.1) + 1
    n_local_search_iters = n_iters - n_random_search_iters

    best_value, best_params = random_search(f, n_params, bounds, n_random_search_iters)
    best_value, best_params, radius_list = random_local_search(
        f, best_value, best_params, n_local_search_iters, n_params, bounds
    )

    return best_params, best_value, radius_list

"""## BFGS"""

######################################
# Forward finite differences
######################################

def forward_finite_diff(f, x, Delta, f_old):
    n  = np.shape(x)[0]
    x  = x.reshape((n, 1))
    dX = np.zeros((n, 1))

    for j in range(n):
        x_d_f = np.copy(x)
        x_d_f[j] = x_d_f[j] + Delta
        dX[j] = (f(x_d_f) - f_old) / Delta
    return dX

#############################
# Line search function
#############################

def line_search_f(direction, x, f, lr, grad_k, f_old, armijo_wolfe=0):
    old_f       = f_old
    new_f       = old_f + 1.
    ls_i        = 0
    lr_i        = 2. * lr
    c_1         = 1e-4
    LS_max_iter = 8

    x_i = x  # If gradient is NaN

    # Armijo line search
    if armijo_wolfe == 1:
        armijo_ = old_f - c_1 * lr_i * grad_k.T @ direction
        while new_f > armijo_ and ls_i < LS_max_iter:
            lr_i /= 2.
            x_i = x - lr_i * direction
            new_f = f(x_i)
            ls_i += 1

    # Naive line search
    elif armijo_wolfe == 0:
        while new_f > old_f and ls_i < LS_max_iter:
            lr_i /= 2.
            x_i = x - lr_i * direction
            new_f = f(x_i)
            ls_i += 1

    return x_i, ls_i, new_f

#############################
# Approximating Hessian
#############################

def Hk_f(x, x_past, grad_i, grad_i_past, Hk_past, Imatrix):
    sk  = x - x_past
    yk  = grad_i - grad_i_past
    rho = 1. / (yk.T @ sk + 1e-7)

    Hinv = (Imatrix - rho * sk @ yk.T) @ Hk_past @ (Imatrix - rho * yk @ sk.T) + rho * sk @ sk.T
    return Hinv

#############################
# First step
#############################

def BFGS_step1(f, x0, n, grad_f, Imatrix, Delta, f_old):
    grad_i      = grad_f(f, x0, Delta, f_old)
    x           = x0 - 1e-8 * grad_i
    f_new       = f(x)
    x_past      = x0.reshape((n, 1))
    grad_i_past = grad_i
    grad_i      = grad_f(f, x, Delta, f_new)
    sk          = x - x_past
    yk          = grad_i - grad_i_past
    Hk_past     = ((yk.T @ sk) / (yk.T @ yk)) * Imatrix
    return Hk_past, grad_i_past, x_past, grad_i, x, f_new

#############################
# Randomized multistart
#############################

def x0_startf(bounds, n_s, N_x):
    """
    Generates `n_s` starting points in an `N_x`-dimensional space within `bounds`
    using uniform random sampling instead of Sobol sequences.
    """
    bounds_l = np.array([bounds[i, 1] - bounds[i, 0] for i in range(len(bounds))])
    sobol_l  = np.random.uniform(0, 1, size=(n_s, N_x))  # Replaced Sobol sequence
    lb_l     = np.array([bounds[i, 0] for i in range(len(bounds))])
    x0_start = lb_l + sobol_l * bounds_l
    return x0_start

###################################
# BFGS for 'global search'
###################################

def BFGS_gs(f, N_x, bounds, max_eval):
    """
    BFGS for global search with line search. Implements a multi-start
    approach with randomized initialization.
    """
    ns       = 5
    lr       = 1.
    grad_f   = forward_finite_diff
    grad_tol = 1e-7

    # Evaluate starting points
    x0_candidates = x0_startf(bounds, ns, N_x)
    f_l           = [f(x0_candidates[xii]) for xii in range(ns)]

    f_eval = ns
    best_point = ['none', 1e15]
    ns_eval = ns
    Delta = np.sqrt(np.finfo(float).eps)

    while len(f_l) >= 1 and f_eval <= max_eval:
        minindex = np.argmin(f_l)
        x0       = x0_candidates[minindex]
        f_old    = f_l[minindex]

        # Remove used candidate
        x0_candidates = x0_candidates.tolist()
        f_l.pop(minindex)
        x0_candidates.pop(minindex)
        x0_candidates = np.asarray(x0_candidates)

        # Initialize problem
        n = np.shape(x0)[0]
        x = np.copy(x0).reshape((n, 1))
        iter_i = 0
        Imatrix = np.identity(n)

        # First step: Gradient descent
        Hk_past, grad_i_past, x_past, grad_i, x, f_old = BFGS_step1(f, x, n, grad_f, Imatrix, Delta, f_old)
        f_eval += 2 * N_x + 2

        # Optimization loop
        first_iter = True
        while np.sum(np.abs(grad_i)) > grad_tol and f_eval < max_eval:
            if not first_iter:
                grad_i = grad_f(f, x, Delta, f_old)
                f_eval += N_x
            first_iter = False

            # Compute Hessian
            Hinv   = Hk_f(x, x_past, grad_i, grad_i_past, Hk_past, Imatrix)
            x_past = x
            Df_i   = Hinv @ grad_i

            # Line search
            x_i, ls_i, new_f = line_search_f(Df_i, x, f, lr, grad_i, f_old)
            f_eval += ls_i

            # Update variables
            grad_i_past = grad_i
            Hk_past     = Hinv
            x           = x_i
            iter_i     += 1
            f_old       = new_f

            if best_point[1] > new_f:
                best_point = [x, new_f]

        # Generate more random starting points if needed
        if len(f_l) <= 0:
            ns_eval += ns
            x0_candidates = x0_startf(bounds, ns_eval, N_x)
            x0_candidates = x0_candidates[(ns_eval - ns):]
            f_l = [f(x0_candidates[xii]) for xii in range(ns)]

    return x, new_f

"""# Test on mathematical functions

## Mathematical test function

This function class contains several mathematical functions for testing. It automatically tracks function evaluations and handles early termination by padding with the best value, or exceeding the budget by cutting off extra evaluations.
"""

###############################################################################
#                           TestFunction Class                                #
###############################################################################
import numpy as np
import pandas as pd

class TestFunction:
    """
    A class for evaluating various test functions for optimization algorithms.
    Supports Rosenbrock, Levy, Rastrigin, Ackley, and 1-norm.
    """

    def __init__(self, func_type, n_x, int_ratio, step=0.2, spacing=None, track_x=False, x_shift=None, bounds=None, int_var=None):
        self.f_list = []
        self.x_list = []
        self.best_f = []

        self.func_type = func_type
        self.n_x = n_x
        self.spacing = step
        self.track_x = track_x
        self.int_ratio = int_ratio
        self.int_var = int_var

        # self.spacing = spacing

        if x_shift is not None:
            self.x_shift = x_shift
        else:
            self.x_shift = np.zeros((n_x, 1))

        if bounds is not None:
            self.lb = bounds[:, 0]
            self.ub = bounds[:, 1]
        else:
            self.lb = np.zeros(n_x)
            self.ub = np.ones(n_x)

        self.dim = n_x
        # int_amount = int(n_x * int_ratio)
        # self.int_var = sorted(np.random.choice(n_x, int_amount, replace=False).tolist())
        # self.cont_var = [i for i in range(n_x) if i not in self.int_var]


    def eval(self, x):
        """
        Evaluate the chosen test function. Valid types are:
        - 'Rosenbrock_f'
        - 'Levy_f'
        - 'Rastrigin_f'
        - 'Ackley_f'
        - '1_norm'
        """
        # Ensure x is a NumPy array of floats before reshaping
        x_arr = np.array(x, dtype=np.float64).reshape((-1, 1)) + self.x_shift

        if self.func_type == 'Rosenbrock_f':
            val = ((1.0 - x_arr)**2).sum() + 100.0 * ((x_arr[1:] - x_arr[:-1]**2)**2).sum()

        elif self.func_type == 'Levy_f':
            w = 1.0 + (x_arr - 1.0) / 4.0
            term1 = np.sin(np.pi * w[0])**2
            term2 = ((w[:-1] - 1.0)**2 * (1.0 + 10.0 * np.sin(np.pi * w[:-1] + 1.0)**2)).sum()
            term3 = (w[-1] - 1.0)**2 * (1.0 + np.sin(2.0 * np.pi * w[-1])**2)
            val = float((term1 + term2 + term3).item())

        elif self.func_type == 'Rastrigin_f':
            val = 10.0 * self.n_x + (x_arr**2 - 10.0 * np.cos(2.0 * np.pi * x_arr)).sum()
            val = float(val)

        elif self.func_type == 'Ackley_f':
            a, b, c = 20.0, 0.2, 2.0 * np.pi
            norm_sq = (x_arr**2).sum()
            term1 = -a * np.exp(-b * np.sqrt(norm_sq / self.n_x))
            term2 = -np.exp(np.cos(c * x_arr).sum() / self.n_x)
            val = float(term1 + term2 + a + np.e)

        elif self.func_type == '1_norm':
            val = float(np.abs(x_arr).sum())
        else:
            raise ValueError(f"Unsupported function type: {self.func_type}")

        self.f_list.append(val)
        if self.track_x:
            self.x_list.append(x_arr.copy())

        return val

    def best_f_list(self):
        """
        Compute best function values up to each evaluation.
        """
        accum_min = []
        current_best = float('inf')
        for val in self.f_list:
            if val < current_best:
                current_best = val
            accum_min.append(current_best)
        self.best_f = accum_min

    def pad_or_truncate(self, n_p):
        """
        Truncate or pad best_f and f_list to length n_p.
        """
        if not self.best_f:
            self.best_f_list()

        best_f_subset = self.best_f[:n_p]
        f_list_subset = self.f_list[:n_p]

        if best_f_subset:
            b_last = best_f_subset[-1]
            self.best_f_c = best_f_subset + [b_last] * (n_p - len(best_f_subset))
        else:
            self.best_f_c = [float('inf')] * n_p

        if f_list_subset:
            l_last = f_list_subset[-1]
            self.f_list_c = f_list_subset + [l_last] * (n_p - len(f_list_subset))
        else:
            self.f_list_c = [float('inf')] * n_p

"""## Plot routine"""

def plot_performance(test_results, algorithms, functions, output_folder, dimensions, start_indices, ratio_key='r20'):
    performance = {}

    for dim, start_idx in zip(dimensions, start_indices):
        dim_key              = f"D{dim}"
        performance[dim_key] = {}

        for func_name in functions:  # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å function
            print(f"\nüîç Plotting: {func_name} (D={dim})")
            for alg in algorithms:
                alg_name = alg.__name__
                trial = test_results[dim_key][ratio_key][func_name]['all means'][alg_name]
                low   = test_results[dim_key][ratio_key][func_name]['q 100']
                high  = test_results[dim_key][ratio_key][func_name]['q 0']

                perf = (high[start_idx:] - trial[start_idx:]) / (high[start_idx:] - low[start_idx:] + 1e-12)
                performance[dim_key].setdefault(alg_name, {})[func_name] = float(np.mean(perf))

            # ‚úÖ Plot for this function
            plt.figure(figsize=(12, 6))
            for alg in algorithms:
                alg_name = alg.__name__
                trial = test_results[dim_key][ratio_key][func_name]['all means'][alg_name]
                upper = test_results[dim_key][ratio_key][func_name]['all 90'][alg_name]
                lower = test_results[dim_key][ratio_key][func_name]['all 10'][alg_name]

                x = np.arange(len(trial))
                plt.plot(trial, lw=2, label=alg_name)
                plt.fill_between(x, lower, upper, alpha=0.2)

            plt.title(f'{func_name} | Dim: {dim}')
            plt.xlabel('Iterations')
            plt.ylabel('Objective Value')
            plt.yscale('log')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.show()

        # ‚úÖ Show scores for all functions
        for func_name in functions:
            print(f"\n--- {dim_key}, {func_name} (starting from evaluation {start_idx}) ---")
            func_data = test_results[dim_key][ratio_key]
            all_mean_trajectories = func_data[func_name]['all means']
            if not all_mean_trajectories:
                print("No algorithm mean trajectories found.")
                continue

            algorithm_names = list(all_mean_trajectories.keys())
            mean_trajectories = list(all_mean_trajectories.values())

            best_mean_trajectory = np.copy(mean_trajectories[0])
            worst_mean_trajectory = np.copy(mean_trajectories[0])
            for traj in mean_trajectories[1:]:
                best_mean_trajectory = np.minimum(best_mean_trajectory, traj)
                worst_mean_trajectory = np.maximum(worst_mean_trajectory, traj)

            for alg_name, mean_traj in all_mean_trajectories.items():
                numerator = worst_mean_trajectory - mean_traj
                denominator = worst_mean_trajectory - best_mean_trajectory
                score = numerator / (denominator + 1e-9) if not np.all(denominator == 0) else np.zeros_like(numerator)
                print(f"Score for {alg_name}:  Mean Score (after start): {np.mean(score):.4f}")

"""



## Test algorithms"""

#‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á int_var ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ dim, int_ratio

import copy
import numpy as np

algorithms = [
    pso_red_f_v2,
    opt_direct,
    opt_BO,
    opt_de,
    opt_SnobFit,
    opt_Bobyqa,
    opt_nelder_mead,
    SS_alg,
    random_search,
    opt_powell,
    BFGS_gs,
    opt_cobyla
]

functions     = ['Rosenbrock_f', 'Levy_f', 'Ackley_f', 'Rastrigin_f', '1_norm']
# dimensions    = [5, 10, 20]
# eval_limits   = [50, 100, 150]
# start_indices = [10, 20, 30]

dimensions    = [5]
eval_limits   = [50]
start_indices = [10]

repetitions   = 5
int_ratios    = [0.2]
step          = 1

test_results = {}

# ‚úÖ Prepare int_var_dict
int_var_dict = {}
for ratio in int_ratios:
    for dim in dimensions:
        int_amount = int(dim * ratio)
        int_var_dict[(ratio, dim)] = sorted(np.random.choice(dim, int_amount, replace=False).tolist())

# ‚úÖ Main loop
for ratio in int_ratios:
    ratio_key = f"r{int(ratio * 100)}"

    for func_name in functions:
        for dim, max_evals, start_idx in zip(dimensions, eval_limits, start_indices):
            dim_key = f"D{dim}"
            bounds = np.array([[-7, 7]] * dim)
            int_var = int_var_dict[(ratio, dim)]

            # ‚úÖ PRINT info block
            print(f"\nüöÄ Function: {func_name} | Dimension: {dim} | int_ratio: {ratio:.1f}")  # <-- ‡πÄ‡∏û‡∏¥‡πà‡∏° print

            test_results.setdefault(dim_key, {})
            test_results[dim_key].setdefault(ratio_key, {})
            test_results[dim_key][ratio_key][func_name] = {
                'all means': {},
                'all 90': {},
                'all 10': {}
            }
            all_results = []

            shifts = np.random.uniform(-6, 6, size=(repetitions, dim))

            for alg in algorithms:
                alg_name = alg.__name__
                print(f"\n== {alg_name} ==")
                print(f"   Integer positions: {int_var}")

                run_results = []
                for rep in range(repetitions):
                    shift = shifts[rep].reshape((dim, 1))

                    t_func = TestFunction(
                        func_type=func_name,
                        n_x=dim,
                        track_x=False,
                        x_shift=shift,
                        int_ratio=ratio,
                        step=step,
                        bounds=bounds,
                        int_var=int_var
                    )

                    if alg_name in ['opt_DYCORS', 'opt_SRBF', 'opt_SOP']:
                        alg(t_func, dim, bounds, max_evals)

                    elif alg_name == 'opt_BO':
                        best_x, best_y = alg(t_func.eval, dim, bounds, max_evals, int_var=int_var)

                    else:
                        alg(t_func.eval, dim, bounds, max_evals)

                    t_func.best_f_list()
                    t_func.pad_or_truncate(max_evals)
                    run_results.append(copy.deepcopy(t_func.best_f_c))

                run_array = np.array(run_results)

                test_results[dim_key][ratio_key][func_name][alg_name]              = run_array
                test_results[dim_key][ratio_key][func_name]['all means'][alg_name] = np.mean(run_array, axis=0)
                test_results[dim_key][ratio_key][func_name]['all 90'][alg_name]    = np.quantile(run_array, 0.9, axis=0)
                test_results[dim_key][ratio_key][func_name]['all 10'][alg_name]    = np.quantile(run_array, 0.1, axis=0)
                all_results.append(run_array)

            combined = np.concatenate(all_results, axis=0)
            test_results[dim_key][ratio_key][func_name]['mean']   = np.mean(combined, axis=0)
            test_results[dim_key][ratio_key][func_name]['median'] = np.median(combined, axis=0)
            test_results[dim_key][ratio_key][func_name]['q 0']    = np.max(combined, axis=0)
            test_results[dim_key][ratio_key][func_name]['q 100']  = np.min(combined, axis=0)

print("\n‚úÖ Finished running all experiments.")

"""## Plots and assessment

This section describes the methodology used to score the performance of optimization algorithms, focusing on their progress after a specific number of initial function evaluations, denoted by the **start index**.

### Benchmarking Process

The benchmarking process involves running a set of optimization algorithms on a chosen objective function across different dimensions and with varying evaluation limits. For each algorithm, dimension, and evaluation limit, multiple independent runs are performed. The performance of each run is tracked as the best function value found so far at each function evaluation.

### Focusing on Performance After the Start Index

To evaluate the algorithms' performance after a certain initial phase, we introduce a **start index** ($s$). For each run, we only consider the sequence of best function values starting from the $s$-th evaluation up to the maximum allowed evaluations. Let $f_{i,j,k}(t)$ be the best function value found by algorithm $i$ in repetition $j$ at the $t$-th evaluation, where $t \ge s$.

### Calculating the Mean Trajectory After the Start Index

For each algorithm $i$, the mean performance trajectory after the start index is calculated by averaging the performance trajectories of all repetitions $j$:

$$
\bar{f}_i(t) = \frac{1}{R} \sum_{j=1}^{R} f_{i,j,k}(t), \quad \text{for } t \ge s
$$

where $R$ is the total number of repetitions. Note that the trajectories $f_{i,j,k}(t)$ are truncated or padded to have a consistent length after the start index, up to the maximum evaluation limit.

### Identifying the Best and Worst Mean Trajectories

For a given dimension and function, we determine the algorithm with the best mean performance and the algorithm with the worst mean performance across all evaluation points after the start index. The best mean trajectory $\bar{f}_{\text{best}}(t)$ and the worst mean trajectory $\bar{f}_{\text{worst}}(t)$ are obtained by element-wise comparison of all the mean trajectories:

$$
\bar{f}_{\text{best}}(t) = \min_{i} \{ \bar{f}_i(t) \}
$$

$$
\bar{f}_{\text{worst}}(t) = \max_{i} \{ \bar{f}_i(t) \}
$$

where the minimum and maximum are taken over all the tested algorithms at each evaluation $t \ge s$.

### Performance Score Calculation

A performance score $S_i(t)$ is calculated for each algorithm $i$ at each evaluation $t \ge s$ using the following equation:

$$
S_i(t) = \frac{\bar{f}_{\text{worst}}(t) - \bar{f}_i(t)}{\bar{f}_{\text{worst}}(t) - \bar{f}_{\text{best}}(t) + \epsilon}
$$

where:

* $\bar{f}_i(t)$ is the mean performance trajectory of algorithm $i$ after the start index.
* $\bar{f}_{\text{worst}}(t)$ is the worst mean performance trajectory across all algorithms after the start index.
* $\bar{f}_{\text{best}}(t)$ is the best mean performance trajectory across all algorithms after the start index.
* $\epsilon$ is a small positive constant (e.g., $10^{-9}$) added to the denominator for numerical stability to avoid division by zero if the best and worst trajectories are identical.

### Interpretation of the Score

The score $S_i(t)$ provides a normalized measure of algorithm $i$'s performance relative to the best and worst performing algorithms after the initial phase:

* A score close to 0 indicates that the algorithm's mean performance is close to the best mean performance at evaluation $t$.
* A score close to 1 indicates that the algorithm's mean performance is close to the worst mean performance at evaluation $t$.
* Scores can potentially fall outside the $[0, 1]$ range if an algorithm temporarily performs better than the current best or worse than the current worst at a given evaluation point.

The overall performance of an algorithm can be summarized by taking the mean of its score trajectory $S_i(t)$ over the considered evaluation range ($t \ge s$).
"""

plot_performance(test_results, algorithms, functions, ".", dimensions, start_indices, ratio_key="r20")

"""# Continuously stirred tank reactor (CSTR) control

![image.png](data:image/png;base64,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)

source: http://apmonitor.com/do/index.php/Main/NonlinearControl

## plots
"""

import numpy as np
import matplotlib.pyplot as plt
from copy import deepcopy
from scipy.integrate import odeint

# Utility function for plotting repeated trajectories
def plot_repeated_trajectories(ax, t, data, desired, color, ylabel, label, ylim=None):
    repetitions = data.shape[0]
    alphas = [(repetitions - float(i)) / repetitions for i in range(repetitions)]
    for i in range(repetitions):
        ax.plot(t, data[i, :], '-', lw=1, color=color, alpha=alphas[i])
    ax.step(t, desired, '--', lw=1.5, color='black', label='Desired')
    ax.set_ylabel(ylabel)
    ax.set_xlabel('Time (min)')
    ax.legend([label, 'Desired'], loc='best')
    if ylim:
        ax.set_ylim(ylim)
    ax.grid(True)

# Training plots
def training_plot(data_plot, data_res, repetitions):
    t = data_plot['t']
    Tc_train = np.array(data_plot['Tc_train'])
    Ca_train = np.array(data_plot['Ca_train'])
    T_train = np.array(data_plot['T_train'])
    Ca_des = data_res['Ca_des']
    T_des = data_res['T_des']

    fig, axes = plt.subplots(3, 1, figsize=(8, 10))

    plot_repeated_trajectories(axes[0], t, Ca_train, Ca_des, 'r', 'A (mol/m¬≥)', 'Concentration of A', ylim=[0.75, 0.95])
    axes[0].set_title('Training Plots')

    plot_repeated_trajectories(axes[1], t, T_train, T_des, 'c', 'T (K)', 'Reactor Temperature', ylim=[317, 335])

    for i in range(repetitions):
        axes[2].step(t[1:], Tc_train[i, :], 'b--', lw=1, alpha=(repetitions - float(i)) / repetitions)
    axes[2].set_ylabel('Cooling T (K)')
    axes[2].set_xlabel('Time (min)')
    axes[2].legend(['Jacket Temperature'], loc='best')
    axes[2].grid(True)

    plt.tight_layout()
    plt.show()

# Convergence plots
def plot_convergence(Xdata, best_Y=None, Objfunc=None):
    if best_Y is None:
        best_Y, f_best = [], 1e8
        for x in Xdata:
            f_val = Objfunc(x, collect_training_data=False)
            f_best = min(f_best, f_val)
            best_Y.append(f_best)
        best_Y = np.array(best_Y)

    distances = np.sqrt(np.sum((np.diff(Xdata, axis=0))**2, axis=1))

    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    axes[0].plot(distances, '-ro')
    axes[0].set_xlabel('Iteration')
    axes[0].set_ylabel('d(x[n], x[n-1])')
    axes[0].set_title('Distance between consecutive samples')
    axes[0].grid(True)

    axes[1].plot(best_Y, '-o')
    axes[1].set_title('Best sample value')
    axes[1].set_xlabel('Iteration')
    axes[1].set_ylabel('Best Y')
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()

# Utility for uncertainty plots
def plot_with_uncertainty(ax, t, data, desired, color, ylabel, label):
    median = np.median(data, axis=1)
    ax.plot(t, median, '-', color=color, lw=2)
    ax.fill_between(t, np.min(data, axis=1), np.max(data, axis=1), color=color, alpha=0.2)
    ax.step(t, desired, '--', color='black', lw=1.5, label='Desired')
    ax.set_ylabel(ylabel)
    ax.set_xlabel('Time (min)')
    ax.legend([label, 'Desired'], loc='best')
    ax.grid(True)

# Control action performance plots
def plot_u_result(u_plt, data_plot, Ca_des, T_des, repetitions):
    t, noise, n = data_plot['t'], data_plot['noise'], data_plot['n']
    u_plt = np.array(u_plt).reshape(-1)

    Ca_dat, T_dat, Tc_dat = [], [], []
    for _ in range(repetitions):
        Ca, T, x = deepcopy(data_plot['Ca_dat']), deepcopy(data_plot['T_dat']), deepcopy(data_plot['x0'])
        Ca_run, T_run = [x[0]], [x[1]]
        for Tc in u_plt:
            y = odeint(cstr, x, [0, t[1]-t[0]], args=(Tc,))[-1]
            x = y + noise * np.random.uniform(-1, 1, 2) * np.array([0.1, 5])
            Ca_run.append(x[0])
            T_run.append(x[1])
        Ca_dat.append(Ca_run)
        T_dat.append(T_run)
        Tc_dat.append(u_plt)

    Ca_dat, T_dat, Tc_dat = np.array(Ca_dat).T, np.array(T_dat).T, np.array(Tc_dat).T

    fig, axes = plt.subplots(3, 1, figsize=(8, 10))

    plot_with_uncertainty(axes[0], t, Ca_dat, Ca_des, 'r', 'A (mol/m¬≥)', 'Concentration of A')
    axes[0].set_title('Control Actions Performance')

    plot_with_uncertainty(axes[1], t, T_dat, T_des, 'c', 'T (K)', 'Reactor Temperature')

    axes[2].step(t[1:], np.median(Tc_dat, axis=1), 'b--', lw=2)
    axes[2].set_ylabel('Cooling T (K)')
    axes[2].set_xlabel('Time (min)')
    axes[2].legend(['Jacket Temperature'], loc='best')
    axes[2].grid(True)

    plt.tight_layout()
    plt.show()

"""## CSTR model

The system used in this tutorial notebook is a Continuous Stirred Tank Reactor (CSTR) described by the following equations

$$\frac{d\text{Ca}}{dt}  = (\text{Ca}_f - \text{Ca})q/V - r_A$$

$$\frac{d\text{Cb}}{dt}  = r_A - \text{Cb}~q/V $$

$$\frac{dT}{dt}   = \frac{q (T_f - T)}{V} + \frac{\Delta H}{(\rho ~C_p)}r_A + \frac{U_A}{(\rho~ V~  Cp)}(Tc-T)$$

with $r_A = k \exp^{(-E/(RT))}\text{Ca}$.

Details of the nomenclature for this system can be found in the code below.
"""

###############
#  CSTR model #
###############

def cstr(x,t,u):

    # ==  Inputs (3) == #
    Tc  = u   # Temperature of cooling jacket (K)

    # == States == #
    Ca = x[0] # Concentration of A in CSTR (mol/m^3)
    T  = x[1] # Temperature in CSTR (K)

    # == Process parameters == #
    Tf     = 350
    q      = 100    # Volumetric Flowrate (m^3/sec)
    Caf    = 1      # Feed Concentration (mol/m^3)
    V      = 100    # Volume of CSTR (m^3)
    rho    = 1000   # Density of A-B Mixture (kg/m^3)
    Cp     = 0.239  # Heat capacity of A-B Mixture (J/kg-K)
    mdelH  = 5e4    # Heat of reaction for A->B (J/mol)
    EoverR = 8750   # E -Activation energy (J/mol), R -Constant = 8.31451 J/mol-K
    k0     = 7.2e10 # Pre-exponential factor (1/sec)
    UA     = 5e4    # U -Heat Transfer Coefficient (W/m^2-K) A -Area - (m^2)
    rA     = k0*np.exp(-EoverR/T)*Ca # reaction rate
    dCadt  = q/V*(Caf - Ca) - rA     # Calculate concentration derivative
    dTdt   = q/V*(Tf - T) \
              + mdelH/(rho*Cp)*rA \
              + UA/V/rho/Cp*(Tc-T)   # Calculate temperature derivative

    # == Return xdot == #
    xdot    = np.zeros(2)
    xdot[0] = dCadt
    xdot[1] = dTdt
    return xdot

def process_operation():
    """
    Performs the 'process operation' as defined in the original code block.
    Returns a dictionary containing the results.
    """
    data_res = {}
    # Initial conditions for the states
    x0             = np.zeros(2)
    x0[0]          = 0.87725294608097
    x0[1]          = 324.475443431599
    data_res['x0'] = x0

    # u_ss = 300.0 # Steady State Initial Condition !!
    # Caf  = 1     # Feed Concentration (mol/m^3)   !!

    # Time interval (min)
    n             = 101 # number of intervals
    Tf            = 25 # process time (min)
    t             = np.linspace(0,Tf,n)
    data_res['t'] = t
    data_res['n'] = n

    # Store results for plotting
    Ca = np.zeros(len(t));      Ca[0]  = x0[0]
    T  = np.zeros(len(t));      T[0]   = x0[1]
    Tc = np.zeros(len(t)-1);

    data_res['Ca_dat'] = copy.deepcopy(Ca)
    data_res['T_dat']  = copy.deepcopy(T)
    data_res['Tc_dat'] = copy.deepcopy(Tc)

    # noise level
    noise             = 0.1
    data_res['noise'] = noise

    # control upper and lower bounds
    data_res['Tc_ub']  = 305
    data_res['Tc_lb']  = 295
    Tc_ub              = data_res['Tc_ub']
    Tc_lb              = data_res['Tc_lb']

    # desired setpoints
    n_1                = int(n/2)
    n_2                = n - n_1
    Ca_des             = [0.8 for i in range(n_1)] + [0.9 for i in range(n_2)]
    T_des              = [330 for i in range(n_1)] + [320 for i in range(n_2)]
    data_res['Ca_des'] = Ca_des
    data_res['T_des']  = T_des

    # normalization for states and actions (for Reinforcement learning section)
    data_res['x_norm'] = np.array([[.8, 315,0, 0],[.1, 10,.1, 20]]) # [mean],[range]
    data_res['u_norm'] = np.array([[10/6],[295]])            #[range/6],[bias]

    return data_res

"""## PID policy"""

##################
# PID controller #
##################

def PID(Ks, x, x_setpoint, e_history):

    Ks    = np.array(Ks)
    Ks    = Ks.reshape(7, order='C')

    # K gains
    KpCa = Ks[0]; KiCa = Ks[1]; KdCa = Ks[2]
    KpT  = Ks[3]; KiT  = Ks[4]; KdT  = Ks[5];
    Kb   = Ks[6]
    # setpoint error
    e = x_setpoint - x
    # control action u_1
    u  = KpCa*e[0] + KiCa*sum(e_history[:,0]) + KdCa*(e[0]-e_history[-1,0])
    u += KpT *e[1] + KiT *sum(e_history[:,1]) + KdT *(e[1]-e_history[-1,1])
    u += Kb
    u  = min(max(u,295),305)

    return u

"""## PID Control test function"""

import numpy as np
import copy
from scipy.integrate import odeint

# PID controller helper function
def compute_pid_control(Ks, x, x_sp, e_history):
    return PID(Ks, x, x_sp, np.array(e_history))

# Objective calculation helper function
def compute_objectives(Ca, T, Tc):
    error = np.abs(Ca) / 0.2 + np.abs(T) / 15
    u_mag = np.abs(Tc - 295) / 100  # normalized magnitude penalty
    u_cha = np.abs(np.diff(Tc)) / 100  # normalized change penalty
    return error, u_mag, u_cha

# Main simulation and control function
def J_ControlCSTR(Ks, data_res, collect_training_data=True):
    # Deep copy to avoid mutation
    Ca = copy.deepcopy(data_res['Ca_dat'])
    T = copy.deepcopy(data_res['T_dat'])
    Tc = copy.deepcopy(data_res['Tc_dat'])
    t = data_res['t']
    x     = copy.deepcopy(data_res['x0'])
    noise = data_res['noise']

    # Setpoints and bounds
    Ca_des, T_des = data_res['Ca_des'], data_res['T_des']
    Tc_lb, Tc_ub  = data_res['Tc_lb'], data_res['Tc_ub']

    e_history = []

    for i in range(len(t) - 1):
        ts = [t[i], t[i + 1]]
        x_sp = np.array([Ca_des[i], T_des[i]])

        # Compute control
        Tc[i] = compute_pid_control(Ks, x, x_sp, [[0, 0]] if i == 0 else e_history)

        # Integrate the system
        y = odeint(cstr, x, ts, args=(Tc[i],))

        # Add stochastic disturbances
        disturbance = noise * np.random.uniform(-1, 1, size=2)
        Ca[i + 1] = y[-1, 0] + disturbance[0] * 0.1
        T[i + 1] = y[-1, 1] + disturbance[1] * 5

        # Update states
        x = [Ca[i + 1], T[i + 1]]

        # Compute tracking errors
        e_history.append(x_sp - x)

    # Calculate objective penalties
    error, u_mag, u_cha = compute_objectives(np.array(e_history)[:, 0], np.array(e_history)[:, 1], Tc)

    if collect_training_data:
        data_res['Ca_train'].append(Ca)
        data_res['T_train'].append(T)
        data_res['Tc_train'].append(Tc)
        data_res['err_train'].append(error)
        data_res['u_mag_train'].append(u_mag)
        data_res['u_cha_train'].append(u_cha)
        data_res['Ks'].append(Ks)

    total_cost = np.sum(error) + np.sum(u_mag) + np.sum(u_cha)

    return total_cost

class PIDTestFunction:
    """
    A class for evaluating PID controller performance
    in a CSTR (Continuous Stirred Tank Reactor) system.
    """

    def __init__(
        self,
        data_res,
        track_x=False,
        bounds=None
    ):
        # Initialize lists to track evaluations
        self.f_list = []
        self.x_list = []
        self.best_f = []

        # Store the data resolution dictionary
        self.data_res = copy.deepcopy(data_res)

        # Reset training data collections
        self.data_res['Ca_train'] = []
        self.data_res['T_train'] = []
        self.data_res['Tc_train'] = []
        self.data_res['err_train'] = []
        self.data_res['u_mag_train'] = []
        self.data_res['u_cha_train'] = []
        self.data_res['Ks'] = []

        # Track x option
        self.track_x = track_x

        # Set bounds if provided, otherwise use default
        if bounds is not None:
            self.lb = bounds[:, 0]
            self.ub = bounds[:, 1]
        else:
            # Default bounds if not specified
            self.lb = np.zeros(3)  # Assuming 3 PID gains
            self.ub = np.ones(3)   # Normalized bounds

        # Dimensionality
        self.n_x = 3  # Typically P, I, D gains
        self.dim = self.n_x
        self.int_var = np.array([], dtype=int)
        self.cont_var = np.arange(0, self.n_x)

    def eval(self, x):
        """
        Evaluate the PID controller performance

        Parameters:
        x : array-like
            PID controller gains [Kp, Ki, Kd]

        Returns:
        float
            Total cost of the controller performance
        """
        # Create a deep copy of the data resolution to avoid mutation
        data_res_copy = copy.deepcopy(self.data_res)

        # Compute the total cost using the J_ControlCSTR function
        val = J_ControlCSTR(x, data_res_copy)

        # Store the function value
        self.f_list.append(val)

        # Track x if required
        if self.track_x:
            self.x_list.append(np.array(x).reshape((-1, 1)))

        return val

    def best_f_list(self):
        """
        Compute best function values up to each evaluation.
        """
        accum_min = []
        current_best = float('inf')
        for val in self.f_list:
            if val < current_best:
                current_best = val
            accum_min.append(current_best)
        self.best_f = accum_min

    def pad_or_truncate(self, n_p):
        """
        Truncate or pad best_f and f_list to length n_p.
        """
        if not self.best_f:
            self.best_f_list()

        best_f_subset = self.best_f[:n_p]
        f_list_subset = self.f_list[:n_p]

        if best_f_subset:
            b_last = best_f_subset[-1]
            self.best_f_c = best_f_subset + [b_last] * (n_p - len(best_f_subset))
        else:
            self.best_f_c = [float('inf')] * n_p

        if f_list_subset:
            l_last = f_list_subset[-1]
            self.f_list_c = f_list_subset + [l_last] * (n_p - len(f_list_subset))
        else:
            self.f_list_c = [float('inf')] * n_p

"""## Test CSTR"""

# Define your optimization algorithms
algorithms = [
    pso_red_f_v2,
    opt_direct,
    opt_BO,
    opt_de,
    opt_SnobFit,
    opt_Bobyqa,
    opt_nelder_mead,
    SS_alg,
    random_search,
    opt_powell,
    BFGS_gs,
    opt_cobyla
]

# Set optimization parameters
dimensions     = [7]  # Example dimensions for PID gains
max_evals_list = [70]
start_indices  = [14]
repetitions    = 5
Tc_lb          = 295

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î ratio ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
ratio = 0.2
ratio_key = f"r{int(ratio * 100)}"

test_results = {}
for dim, max_evals in zip(dimensions, max_evals_list):
    dim_key = f"D{dim}"
    test_results[dim_key] = {}

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ ratio_key ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    test_results[dim_key][ratio_key] = {}

    # Assume PID gains bounds between 0 and 10
    bounds = np.array([[0.,10./0.2]]*3 + [[0.,10./15]]*3 + [[Tc_lb-20,Tc_lb+20]])

    test_results[dim_key][ratio_key]['J_ControlCSTR'] = {
        'all means': {},
        'all 90': {},
        'all 10': {}
    }

    all_results = []

    # Random initial shifts for PID parameters
    shifts = np.random.uniform(0, 10, size=(repetitions, dim))

    for alg in algorithms:
        alg_name = alg.__name__
        print(f"== {alg_name} optimizing J_ControlCSTR ({dim} parameters) ==")

        run_results = []

        for rep in range(repetitions):
            # Prepare initial data resolution
            data_res = process_operation()

            # Create PIDTestFunction instance
            pid_test_func = PIDTestFunction(
                data_res,
                track_x=False,
                bounds=bounds
            )

            # Define objective function wrapper using PIDTestFunction
            def objective(Ks):
                return pid_test_func.eval(Ks)

            # Run optimization algorithm
            if alg_name in ['opt_DYCORS', 'opt_SRBF', 'opt_SOP']:
                alg(objective, dim, bounds, max_evals)

            else:
                alg(objective, dim, bounds, max_evals)

            # Store best result
            # Use the initial shifted parameters as a starting point
            pid_test_func.best_f_list()
            pid_test_func.pad_or_truncate(max_evals)
            run_results.append(copy.deepcopy(pid_test_func.best_f_c))

        run_array = np.array(run_results)

        test_results[dim_key][ratio_key]['J_ControlCSTR'][alg_name] = run_array
        test_results[dim_key][ratio_key]['J_ControlCSTR']['all means'][alg_name] = np.mean(run_array, axis=0)
        test_results[dim_key][ratio_key]['J_ControlCSTR']['all 90'][alg_name]    = np.quantile(run_array, 0.9, axis=0)
        test_results[dim_key][ratio_key]['J_ControlCSTR']['all 10'][alg_name]    = np.quantile(run_array, 0.1, axis=0)

        all_results.append(run_array)

    combined = np.concatenate(all_results)
    test_results[dim_key][ratio_key]['J_ControlCSTR']['mean']   = np.mean(combined, axis=0)
    test_results[dim_key][ratio_key]['J_ControlCSTR']['median'] = np.median(combined, axis=0)
    test_results[dim_key][ratio_key]['J_ControlCSTR']['q 0']    = np.max(combined, axis=0)
    test_results[dim_key][ratio_key]['J_ControlCSTR']['q 100']  = np.min(combined, axis=0)

print(f"\n=== Finished running all J_ControlCSTR optimization experiments (ratio={ratio}) ===")

"""## Test algorithms on CSTR"""

plot_performance(test_results, algorithms, ['J_ControlCSTR'], ".", dimensions, start_indices, ratio_key='r20')

"""# Test on high dimensional correlated functions

## High dimensional linearly corelated functions
"""

import numpy as np

###############################################################################
#                               LatentFunction Class                          #
###############################################################################

class LatentFunction:
    """
    A class for evaluating test functions where the input to the core
    nonlinear function is a linear combination of the actual input variables.
    Supports Rosenbrock, Levy, Rastrigin, Ackley, and 1-norm as latent functions.
    """

    def __init__(
        self,
        latent_func_type,
        n_x,
        int_ratio,
        step=1,
        spacing=None,
        track_x=False,
        x_shift=None,
        bounds=None,
        linear_transform=None,
        int_var=None
    ):
        self.f_list = []
        self.x_list = []
        self.best_f = []

        self.latent_func_type = latent_func_type
        self.n_x = n_x
        self.track_x = track_x
        self.spacing = step
        self.int_ratio = int_ratio
        self.int_var = int_var

        if x_shift is not None:
            self.x_shift = x_shift.reshape((n_x, 1))
        else:
            self.x_shift = np.zeros((n_x, 1))

        if bounds is not None:
            self.lb = bounds[:, 0].reshape((n_x, 1))
            self.ub = bounds[:, 1].reshape((n_x, 1))
        else:
            self.lb = np.zeros((n_x, 1))
            self.ub = np.ones((n_x, 1))

        self.dim = n_x
        self.int_var = np.array([], dtype=int)
        self.cont_var = np.arange(0, n_x)

        if linear_transform is not None:
            self.linear_transform = linear_transform
            self.n_latent = linear_transform.shape[0]
        else:
            self.linear_transform = np.eye(n_x)
            self.n_latent = n_x

    def _evaluate_latent_function(self, z):
        n_latent = self.n_latent

        if self.latent_func_type == 'Rosenbrock_f':
            val = ((1.0 - z)**2).sum() + 100.0 * ((z[1:] - z[:-1]**2)**2).sum()

        elif self.latent_func_type == 'Levy_f':
            w = 1.0 + (z - 1.0) / 4.0
            term1 = np.sin(np.pi * w[0])**2
            term2 = ((w[:-1] - 1.0)**2 * (1.0 + 10.0 * np.sin(np.pi * w[:-1] + 1.0)**2)).sum()
            term3 = (w[-1] - 1.0)**2 * (1.0 + np.sin(2.0 * np.pi * w[-1])**2)
            val = float(term1 + term2 + term3)

        elif self.latent_func_type == 'Rastrigin_f':
            val = 10.0 * n_latent + (z**2 - 10.0 * np.cos(2.0 * np.pi * z)).sum()
            val = float(val)

        elif self.latent_func_type == 'Ackley_f':
            a, b, c = 20.0, 0.2, 2.0 * np.pi
            norm_sq = (z**2).sum()
            term1 = -a * np.exp(-b * np.sqrt(norm_sq / n_latent))
            term2 = -np.exp(np.cos(c * z).sum() / n_latent)
            val = float(term1 + term2 + a + np.e)

        elif self.latent_func_type == '1_norm':
            val = float(np.abs(z).sum())
        else:
            raise ValueError(f"Unsupported latent function type: {self.latent_func_type}")

        return val

    def eval(self, x):
        x_arr = np.array(x).reshape((self.n_x, 1)) + self.x_shift
        z = self.linear_transform @ x_arr

        val = self._evaluate_latent_function(z)

        self.f_list.append(val)
        if self.track_x:
            self.x_list.append(x_arr.copy())

        return val

    def best_f_list(self):
        accum_min = []
        current_best = float('inf')
        for val in self.f_list:
            if val < current_best:
                current_best = val
            accum_min.append(current_best)
        self.best_f = accum_min

    def pad_or_truncate(self, n_p):
        if not self.best_f:
            self.best_f_list()

        best_f_subset = self.best_f[:n_p]
        f_list_subset = self.f_list[:n_p]

        if best_f_subset:
            b_last = best_f_subset[-1]
            self.best_f_c = best_f_subset + [b_last] * (n_p - len(best_f_subset))
        else:
            self.best_f_c = [float('inf')] * n_p

        if f_list_subset:
            l_last = f_list_subset[-1]
            self.f_list_c = f_list_subset + [l_last] * (n_p - len(f_list_subset))
        else:
            self.f_list_c = [float('inf')] * n_p

if __name__ == '__main__':
    n_x = 10
    latent_dim = 5  # The dimension of the input to the latent function

    # Define missing variables
    ratio = 0.2  # int_ratio
    step = 1     # step size
    int_var = None  # or specific indices like [0, 2, 4]

    # Define a random linear transformation matrix (n_latent x n_x)
    linear_transform_matrix = np.random.rand(latent_dim, n_x)

    # Example usage with Rosenbrock as the latent function
    latent_rosenbrock = LatentFunction(
        latent_func_type='Rosenbrock_f',
        n_x=n_x,
        track_x=True,
        int_ratio=ratio,
        step=step,
        int_var=int_var,
        x_shift=np.ones(n_x),
        bounds=np.array([[-5, 5]] * n_x),
        linear_transform=linear_transform_matrix
    )

    # Evaluate the function at a sample point
    x_sample = np.random.rand(n_x) * 10 - 5
    value = latent_rosenbrock.eval(x_sample)
    print(f"Evaluated Latent Rosenbrock at {x_sample}: {value}")

    # Evaluate multiple times
    for _ in range(20):
        x_sample = np.random.rand(n_x) * 10 - 5
        latent_rosenbrock.eval(x_sample)

    # Get the best function values found
    latent_rosenbrock.best_f_list()
    print(f"Best function values over evaluations: {latent_rosenbrock.best_f}")

    # Pad or truncate the best function values list
    latent_rosenbrock.pad_or_truncate(30)
    print(f"Padded/Truncated best function values: {latent_rosenbrock.best_f_c}")

"""## Test algorithm"""

# Define your optimization algorithms and functions
#‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á int_var ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ dim, int_ratio

import copy
import numpy as np

import copy
import numpy as np

# --- Algorithm list ---
algorithms = [
    pso_red_f_v2,
    opt_direct,
    opt_BO,
    opt_de,
    opt_SnobFit,
    opt_Bobyqa,
    opt_nelder_mead,
    SS_alg,
    random_search,
    opt_powell,
    BFGS_gs,
    opt_cobyla
]

# --- Configuration ---
functions     = ['Rosenbrock_f', 'Levy_f', 'Ackley_f', 'Rastrigin_f', '1_norm']
dimensions    = [2, 5, 10]
eval_limits   = [60, 150, 300]
start_indices = [12, 30, 60]
repetitions   = 5
int_ratios    = [0.2]  # or multiple ratios if needed
step          = 1

test_results_latent = {}

# ‚úÖ ‡∏™‡∏∏‡πà‡∏° int_var ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ (ratio, dim)
int_var_dict = {}
for ratio in int_ratios:
    for dim in dimensions:
        int_amount = int(dim * ratio)
        int_var_dict[(ratio, dim)] = sorted(np.random.choice(dim, int_amount, replace=False).tolist())

# ‚úÖ Main experiment loop
for ratio in int_ratios:
    ratio_key = f"r{int(ratio * 100)}"

    for func_name in functions:
        for dim, max_evals, start_idx in zip(dimensions, eval_limits, start_indices):
            dim_key = f"D{dim}"
            bounds = np.array([[-7, 7]] * dim)
            int_var = int_var_dict[(ratio, dim)]

            print(f"\nüöÄ Function: {func_name} | Dimension: {dim} | int_ratio: {ratio:.1f}")

            test_results_latent.setdefault(dim_key, {})
            test_results_latent[dim_key].setdefault(ratio_key, {})
            test_results_latent[dim_key][ratio_key][func_name] = {
                'all means': {},
                'all 90': {},
                'all 10': {}
            }

            all_results = []
            shifts = np.random.uniform(-6, 6, size=(repetitions, dim))

            for alg in algorithms:
                alg_name = alg.__name__
                print(f"== {alg_name} ==")
                print(f"   Integer positions: {int_var}")

                run_results = []
                for rep in range(repetitions):
                    shift = shifts[rep].reshape((dim, 1))
                    latent_dim = dim * 3
                    linear_transform = np.random.uniform(-1, 1, size=(latent_dim, dim))

                    t_func = LatentFunction(
                        latent_func_type=func_name,
                        n_x=dim,
                        track_x=False,
                        x_shift=shift,
                        int_ratio=ratio,
                        step=step,
                        bounds=bounds,
                        int_var=int_var,
                        linear_transform=linear_transform
                    )

                    if alg_name in ['opt_DYCORS', 'opt_SRBF', 'opt_SOP']:
                        alg(t_func, dim, bounds, max_evals)
                    elif alg_name == 'opt_BO':
                        best_x, best_y = alg(t_func.eval, dim, bounds, max_evals, int_var=int_var)
                    else:
                        alg(t_func.eval, dim, bounds, max_evals)

                    t_func.best_f_list()
                    t_func.pad_or_truncate(max_evals)
                    run_results.append(copy.deepcopy(t_func.best_f_c))

                run_array = np.array(run_results)

                test_results_latent[dim_key][ratio_key][func_name][alg_name]              = run_array
                test_results_latent[dim_key][ratio_key][func_name]['all means'][alg_name] = np.mean(run_array, axis=0)
                test_results_latent[dim_key][ratio_key][func_name]['all 90'][alg_name]    = np.quantile(run_array, 0.9, axis=0)
                test_results_latent[dim_key][ratio_key][func_name]['all 10'][alg_name]    = np.quantile(run_array, 0.1, axis=0)
                all_results.append(run_array)

            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°
            combined = np.concatenate(all_results, axis=0)
            test_results_latent[dim_key][ratio_key][func_name]['mean']   = np.mean(combined, axis=0)
            test_results_latent[dim_key][ratio_key][func_name]['median'] = np.median(combined, axis=0)
            test_results_latent[dim_key][ratio_key][func_name]['q 0']    = np.max(combined, axis=0)
            test_results_latent[dim_key][ratio_key][func_name]['q 100']  = np.min(combined, axis=0)

print("\n‚úÖ Finished running all LatentFunction experiments.")

"""## Plot and assessment"""

plot_performance(test_results_latent, algorithms, functions, ".", dimensions, start_indices,ratio_key="r20" )

